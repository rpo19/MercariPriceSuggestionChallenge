{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_oO4ZyYU00Y"
   },
   "source": [
    "### Mercari Price \n",
    "The files consist of a list of product listings. These files are tab-delimited.\n",
    "\n",
    "Fields:\n",
    "- train_id or test_id - the id of the listing\n",
    "\n",
    "- name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid  leakage. These removed prices are represented as [rm]\n",
    "\n",
    "- item_condition_id - the condition of the items provided by the seller\n",
    "\n",
    "- category_name - category of the listing\n",
    "\n",
    "- brand_name\n",
    "\n",
    "- price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.\n",
    "\n",
    "- shipping - 1 if shipping fee is paid by seller and 0 by buyer\n",
    "\n",
    "- item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTBm7hc-CoRA"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZWiRNuUU00w",
    "outputId": "069065c8-69f2-4048-b491-bf17d8879fee"
   },
   "outputs": [],
   "source": [
    "! pip install pydot graphviz emoji transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Concatenate, Flatten, Dropout, LSTM, GlobalMaxPool1D, GRU, Bidirectional, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import emoji\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, pipeline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove(\"no\")\n",
    "\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0ETrpfjg_ha"
   },
   "outputs": [],
   "source": [
    "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "\n",
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    return K.sqrt(msle(y_true, y_pred))\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRXV0oKS-Yn3"
   },
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "ckhXGgT8-XzG",
    "outputId": "276e483d-5b32-4839-bf01-edc4572b9c38"
   },
   "outputs": [],
   "source": [
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "# check if in colab\n",
    "if RunningInCOLAB and not os.path.isdir('/content/gdrive'):\n",
    "    print(\"Running in colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    colab_root = '/content/drive'\n",
    "      \n",
    "if RunningInCOLAB:\n",
    "    root_dir = \"/content/gdrive/My Drive/\"\n",
    "    base_dir = root_dir + 'project-mercari-price/'\n",
    "    if not os.path.isdir(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "else:\n",
    "    root_dir= os.getcwd()\n",
    "    base_dir = root_dir\n",
    "\n",
    "os.chdir(base_dir)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tMt4O6n3yTD"
   },
   "source": [
    "## Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXCUW4PtU00k"
   },
   "outputs": [],
   "source": [
    "dataset_downloaded_path = os.path.join(base_dir, \"dataset_downloaded.ignore\")\n",
    "dataset_downloaded = os.path.isfile(dataset_downloaded_path)\n",
    "dataset_downloaded\n",
    "\n",
    "if not dataset_downloaded:\n",
    "  # install kaggle to download dataset\n",
    "  ! pip install kaggle python-dotenv\n",
    "\n",
    "# set to True if you want to save kaggle credentials into a .env file\n",
    "persist_credentials = False\n",
    "\n",
    "if not dataset_downloaded:\n",
    "  # create .env file containing KAGGLE_USER and KAGGLE_KEY\n",
    "    kaggle_env = os.path.join(base_dir, '.env')\n",
    "    if not os.path.isfile(kaggle_env):\n",
    "        with open(kaggle_env, 'w') as envfile:\n",
    "            kaggle_user = input(\"Insert kaggle username\")\n",
    "            kaggle_key = input(\"Insert kaggle key; generate one from kaggle account\")\n",
    "        if persist_credentials:\n",
    "            envfile.write(f\"\"\"\n",
    "            KAGGLE_USERNAME={kaggle_user}\n",
    "            KAGGLE_KEY={kaggle_key}\n",
    "            \"\"\")\n",
    "\n",
    "        # set env vars\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = kaggle_user\n",
    "        os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
    "\n",
    "        del kaggle_user\n",
    "        del kaggle_key\n",
    "\n",
    "if not dataset_downloaded:\n",
    "  # loading env vars if .env file exists\n",
    "    if os.path.isfile(kaggle_env):\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(dotenv_path=kaggle_env)\n",
    "    print(os.environ.get(\"KAGGLE_USERNAME\"))\n",
    "\n",
    "if not dataset_downloaded:\n",
    "    # download and extract dataset\n",
    "    ! kaggle competitions download -c mercari-price-suggestion-challenge\n",
    "\n",
    "    # create file so that we know we already downloaded\n",
    "    with open(dataset_downloaded_path, 'w') as dd_file:\n",
    "        dataset_downloaded = True\n",
    "        dd_file.write(\"\")\n",
    "\n",
    "    print('cwd: ', os.getcwd())\n",
    "    \n",
    "    os.listdir()\n",
    "\n",
    "if not dataset_downloaded:\n",
    "    ! 7z x train.tsv.7z\n",
    "    ! 7z x test.tsv.7z\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pyEueRK3yTL"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5Vg17INU00x"
   },
   "outputs": [],
   "source": [
    "dtypes={\n",
    "    'name': 'string',\n",
    "    'item_condition_id': 'int32',\n",
    "    'category_name': 'string',\n",
    "    'brand_name': 'string',\n",
    "    'price': 'float',\n",
    "    'shipping': 'int32',\n",
    "    'item_description': 'string'\n",
    "}\n",
    "data = pd.read_csv(\"train.tsv\", sep='\\t', dtype=dtypes)\n",
    "data = data.drop(columns=[\"train_id\"])\n",
    "print(data.dtypes)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWgi5SxW9SsE"
   },
   "outputs": [],
   "source": [
    "dtypes={\n",
    "    'name': 'string',\n",
    "    'item_condition_id': 'int32',\n",
    "    'category_name': 'string',\n",
    "    'brand_name': 'string',\n",
    "    'price': 'float',\n",
    "    'shipping': 'int32',\n",
    "    'item_description': 'string'\n",
    "}\n",
    "test = pd.read_csv(\"test.tsv\", sep='\\t', dtype=dtypes)\n",
    "test = test.drop(columns=[\"test_id\"])\n",
    "print(test.dtypes)\n",
    "print(test.shape)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvujMFFXCxL8"
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    print(\"number of null value in {} : {}\".format(column,data[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NltZr2wAdYc"
   },
   "outputs": [],
   "source": [
    "for column in test.columns:\n",
    "    print(\"number of null value in {} : {}\".format(column,test[column].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtxgmC-LHnmN"
   },
   "outputs": [],
   "source": [
    "data[\"category_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOYErCoVHnmO"
   },
   "outputs": [],
   "source": [
    "data[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw5N2UjxiPcW"
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Handle missing values and wrong prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnzdF-qLrlGI"
   },
   "source": [
    "https://www.mercari.com/us/help_center/article/69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyUE4a4Jp4E9"
   },
   "outputs": [],
   "source": [
    "len(data[data[\"price\"]<5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCbqNKgKp7f2"
   },
   "outputs": [],
   "source": [
    "data=data[data[\"price\"]>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxBs1Xr4AlsD"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"item_description\"].notna()]\n",
    "data[\"brand_name\"] = data[\"brand_name\"].fillna(value=\"NA\")\n",
    "data[\"category_name\"] = data[\"category_name\"].fillna(value=\"NA\")\n",
    "# see warnings -> inplace?\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUBsIEJqC0cg"
   },
   "outputs": [],
   "source": [
    "test[\"brand_name\"] = test[\"brand_name\"].fillna(value=\"NA\")\n",
    "test[\"category_name\"] = test[\"category_name\"].fillna(value=\"NA\")\n",
    "# see warnings -> inplace?\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNY29qP2HnmS"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m7ykQvijKp8"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkGhq22YJJGf"
   },
   "outputs": [],
   "source": [
    "data[\"item_description\"]=data[\"item_description\"].str.lower()\n",
    "data[\"name\"]=data[\"name\"].str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqevrhyRC7yy"
   },
   "outputs": [],
   "source": [
    "test[\"item_description\"]=test[\"item_description\"].str.lower()\n",
    "test[\"name\"]=test[\"name\"].str.lower()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WynSk_Mdj44"
   },
   "outputs": [],
   "source": [
    "tweetTokenizer = TweetTokenizer()\n",
    "\n",
    "def list_to_str(l):\n",
    "   return ' '.join([str(elem) for elem in l])\n",
    "   \n",
    "def textCleanup(df, flag=True):\n",
    "  df=df.to_frame(name=\"str\")\n",
    "  #df[\"clean\"] = df[\"str\"].progress_apply(text_to_word_sequence)   # 20 secondi\n",
    "  global tweetTokenizer\n",
    "  df[\"clean\"] = df[\"str\"].progress_apply(tweetTokenizer.tokenize) # 2 minutes but correctly handles emojis\n",
    "\n",
    "  # punct and stop words\n",
    "  \n",
    "\n",
    "  lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "  df[\"clean\"] = df[\"clean\"].progress_apply(lambda sentence : [lemmatizer.lemmatize(word) for word in sentence if word not in stop_words]) # 10 secondi\n",
    "  df[\"clean\"] = df[\"clean\"].progress_apply(lambda sentence:\n",
    "                                           [w for w in sentence if w\n",
    "                                              not in string.punctuation\n",
    "                                              and w not in stop_words and (len(w)>1 or w.isdigit()) and w not in emoji.UNICODE_EMOJI]) # 18 s\n",
    "  plot_common_tokens(df[\"clean\"], \"Most Common Tokens without StopWords\", n=20)\n",
    "  if flag:\n",
    "    df[\"clean\"] = df[\"clean\"].progress_apply(list_to_str) # 6 secondi\n",
    "  return df[\"clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwTCeUoL6Rq7"
   },
   "outputs": [],
   "source": [
    "def preprocessData(data):\n",
    "  print('description clean up')\n",
    "  data[\"item_description_clean\"] = textCleanup(data[\"item_description\"]) \n",
    "\n",
    "  print('name clean up')\n",
    "  data[\"name_clean\"] = textCleanup(data[\"name\"])\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JELRL6ipo0D"
   },
   "outputs": [],
   "source": [
    "def flat_list(l):\n",
    "    return  [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMSrHLKdpWQL"
   },
   "outputs": [],
   "source": [
    "def plot_common_tokens(tokens, title, n=20):\n",
    "    sentences = (list(itertools.chain(tokens)))\n",
    "    flat_sentences = flat_list(sentences)\n",
    "    counts = Counter(flat_sentences)\n",
    "    #print(counts.most_common(30))\n",
    "    common_words = [word[0] for word in counts.most_common(n)]\n",
    "    common_counts = [word[1] for word in counts.most_common(n)]\n",
    "    fig = plt.figure(figsize=(18,6))\n",
    "    sns.barplot(x=common_words, y=common_counts)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TZbvig1qHceY"
   },
   "outputs": [],
   "source": [
    "data = preprocessData(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE3PL_bmDNjK"
   },
   "outputs": [],
   "source": [
    "test = preprocessData(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC4xOHj_0zow"
   },
   "source": [
    "# Load preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b38isOpV0ejM",
    "outputId": "829f5a28-21f1-49f4-e806-f57d752a625e"
   },
   "outputs": [],
   "source": [
    "# save preprocessed data train\n",
    "force_overwrite = False\n",
    "fname = 'train_preprocess.npy'\n",
    "\n",
    "if force_overwrite or not os.path.isfile(fname):\n",
    "  print(\"saving preprocess train data\")\n",
    "  data.to_pickle(fname)\n",
    "else:\n",
    "  print(\"loading existing preprocess train data\")\n",
    "  data = pd.read_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LJ_alCc1znc"
   },
   "outputs": [],
   "source": [
    "force_overwrite = False\n",
    "fname = 'test_preprocess.npy'\n",
    "\n",
    "if force_overwrite or not os.path.isfile(fname):\n",
    "  print(\"saving preprocess test data\")\n",
    "  test.to_pickle(fname)\n",
    "else:\n",
    "  print(\"loading existing preprocess test data\")\n",
    "  test = pd.read_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62qoDa8Fwr8U"
   },
   "source": [
    "# Normal distribution as regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPeuFNoZxUH8"
   },
   "outputs": [],
   "source": [
    "train_normal, validation_normal = train_test_split(data, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDD1QfgmxyeX",
    "outputId": "573b266f-976b-40dc-dcde-a8cbc71bff9f"
   },
   "outputs": [],
   "source": [
    "mean = train_normal[\"price\"].mean()\n",
    "std = train_normal[\"price\"].std()\n",
    "\n",
    "print(f'mean: {mean}, std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55Nu9LA5zEeu"
   },
   "outputs": [],
   "source": [
    "y_true_normal = validation_normal[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL2h1oxox72v"
   },
   "outputs": [],
   "source": [
    "y_pred_normal = np.random.normal(mean, std, validation_normal.shape[0])\n",
    "# remoing negative values\n",
    "y_pred_normal[y_pred_normal < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMCV-MFXybI6",
    "outputId": "70921438-072c-40f7-d112-622c08cdb954"
   },
   "outputs": [],
   "source": [
    "res = {\n",
    "    'rmsle': np.sqrt(sklearn.metrics.mean_squared_log_error(y_true_normal, y_pred_normal)),\n",
    "    'mae': sklearn.metrics.mean_absolute_error(y_true_normal, y_pred_normal),\n",
    "    'mse': sklearn.metrics.mean_squared_error(y_true_normal, y_pred_normal),\n",
    "    'rmse': np.sqrt(sklearn.metrics.mean_squared_error(y_true_normal, y_pred_normal))\n",
    "}\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsy2na5lR4a7"
   },
   "source": [
    "{'mae': 32.10479458350734,\n",
    " 'mse': 2454.257906180274,\n",
    " 'rmse': 49.540467359324275,\n",
    " 'rmsle': 1.8666666215367753}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYZRBnbN0sv-"
   },
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trBPru1qEK5b"
   },
   "outputs": [],
   "source": [
    "# TODO ensure it is correct or use a well tested alternative like sklearn (found problems with dimensions)\n",
    "class LabelEncoder:\n",
    "  \"\"\"\n",
    "  Simple single dimension label encoder class able to handle\n",
    "  unknown values \n",
    "  \"\"\"\n",
    "  def __init__(self, unknown = 0, invUnknown = 'unknown'):\n",
    "    self.leDict = {}\n",
    "    self.invDict = {}\n",
    "    self.unknown = unknown\n",
    "    self.invUnknown = invUnknown\n",
    "\n",
    "  def fit(self, data):\n",
    "    vci = pd.value_counts(data).index\n",
    "    self.leDict = dict(zip(vci, range(1, len(vci)+1)))\n",
    "    self.invDict = dict(zip(range(1, len(vci)+1), vci))\n",
    "\n",
    "  def transform1(self, item):\n",
    "    return self.leDict.get(item, self.unknown)\n",
    "\n",
    "  def transform(self, data):\n",
    "    return data.apply(lambda item: self.transform1(item))\n",
    "\n",
    "  def inverse_transform1(self, item):\n",
    "    return self.invDict.get(item, self.invUnknown)\n",
    "\n",
    "  def inverse_transform(self, data):\n",
    "    return data.apply(lambda item: self.inverse_transform1(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO4EL__E3yTS"
   },
   "outputs": [],
   "source": [
    "cat_le = LabelEncoder()\n",
    "cat_le.fit(data[\"category_name\"])\n",
    "\n",
    "data[\"category_name_l\"] = cat_le.transform(data[\"category_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfiZte2s3yTS"
   },
   "outputs": [],
   "source": [
    "brand_le = LabelEncoder()\n",
    "brand_le.fit(data[\"brand_name\"])\n",
    "\n",
    "data[\"brand_name_l\"] = brand_le.transform(data[\"brand_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whePTuwQ7n1l"
   },
   "source": [
    "# Test Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kzLzFGi7hA0"
   },
   "outputs": [],
   "source": [
    "test[\"category_name_l\"] = cat_le.transform(test[\"category_name\"])\n",
    "test[\"brand_name_l\"] = brand_le.transform(test[\"brand_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl7czt1455gD"
   },
   "source": [
    "# Categorical only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmlAIViy54d7"
   },
   "outputs": [],
   "source": [
    "train_cat, validation_cat = train_test_split(data[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\", \"price\"]], test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEtjrzW86PpZ"
   },
   "outputs": [],
   "source": [
    "def getModelCat():\n",
    "    inputA = Input(shape=(4,))\n",
    "\n",
    "    x = Dense(32, activation='relu')(inputA)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputA, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oANXklrj6P2g",
    "outputId": "4bd2bd29-9f48-4459-cb8a-41f7c0a67945"
   },
   "outputs": [],
   "source": [
    "model_cat = getModelCat()\n",
    "model_cat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "hSj_KBll9wWp",
    "outputId": "4dd468f4-d016-416a-f2f7-caa90125d622"
   },
   "outputs": [],
   "source": [
    "plot_model(model_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psa0HZfX9wWp"
   },
   "outputs": [],
   "source": [
    "model_cat.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18BgFwva-eT7"
   },
   "outputs": [],
   "source": [
    "inputA_train_cat = train_cat[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnpfLh6d-q8r"
   },
   "outputs": [],
   "source": [
    "inputA_validation_cat = validation_cat[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rwMERNQ-75F"
   },
   "outputs": [],
   "source": [
    "y_train_cat = train_cat[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsJx3Uzt_Apd"
   },
   "outputs": [],
   "source": [
    "y_validation_cat = validation_cat[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJual7EL9wWp",
    "outputId": "4b5cbf2a-779f-436e-f23c-016678cf0af3"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "history_cat = model_cat.fit(x=inputA_train_cat, y=y_train_cat,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(inputA_validation_cat, y_validation_cat),\n",
    "                    callbacks=[callback],\n",
    "                    batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1RY6BUtBxNh"
   },
   "source": [
    "10/10 2261/2261 [==============================] - 5s 2ms/step - loss: 0.6769 - mse: 1529.0349 - mae: 15.5510 - root_mean_squared_error: 39.1006 - mean_squared_logarithmic_error: 0.4589 - root_mean_squared_logarithmic_error: 0.6769 - val_loss: 0.6703 - val_mse: 1456.1018 - val_mae: 15.3929 - val_root_mean_squared_error: 38.1589 - val_mean_squared_logarithmic_error: 0.4501 - val_root_mean_squared_logarithmic_error: 0.6705\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "sfUDgH0D9wWq",
    "outputId": "5b554d2f-2f33-402a-eacc-68c56f5f80f7"
   },
   "outputs": [],
   "source": [
    "histDf_cat = pd.DataFrame(history_cat.history)\n",
    "histDf_cat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EHmSypid9wWq",
    "outputId": "6da742c3-e4c4-44cc-fbbe-6bf92df25550"
   },
   "outputs": [],
   "source": [
    "histDf_cat.plot(y=[\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al4hG-g33lq8"
   },
   "source": [
    "# Keras Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3fBpRMpHJoh"
   },
   "outputs": [],
   "source": [
    "# tokenize with keras; it also does some encoding\n",
    "def tokenizeData(df, description, name, tokenizer= None, texts= None):\n",
    "  if tokenizer == None:\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    print(\"fit tokenizer\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "  \n",
    "  print('tokenize description')\n",
    "  df[\"item_description_t\"]=tokenizer.texts_to_sequences(df[description])\n",
    "\n",
    "  print('tokenize name')\n",
    "  df[\"name_t\"]=tokenizer.texts_to_sequences(df[name])\n",
    "  return df, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hzwtYpKHon7"
   },
   "outputs": [],
   "source": [
    "texts = np.hstack([data[\"item_description_clean\"], data[\"name_clean\"]])\n",
    "\n",
    "data_keras, tokenizer = tokenizeData(data ,\"item_description_clean\" ,\"name_clean\", None, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JClIo-Qt4eS-"
   },
   "outputs": [],
   "source": [
    "#data_keras_bk = data_keras\n",
    "test_keras_bk = test_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc1xhJ9nHnAL"
   },
   "outputs": [],
   "source": [
    "data_keras=data_keras[[\"item_condition_id\",\"shipping\",\"category_name_l\",\"brand_name_l\", \"item_description_t\", \"name_t\", \"price\"]]\n",
    "data_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClNGIbYoE416"
   },
   "outputs": [],
   "source": [
    "test_keras, _ = tokenizeData(test ,\"item_description\", \"name\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srQCadR3Jvfu"
   },
   "outputs": [],
   "source": [
    "test_keras=test[[\"item_condition_id\",\"shipping\",\"category_name_l\",\"brand_name_l\", \"item_description_t\", \"name_t\"]]\n",
    "test_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siBwn0ntHnmY"
   },
   "outputs": [],
   "source": [
    "train_keras, validation_keras = train_test_split(data_keras, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W0A95PQEDb_"
   },
   "outputs": [],
   "source": [
    "vocab_size= len(tokenizer.word_index)+1\n",
    "print(vocab_size)\n",
    "# clean con trattamento emoji 193300\n",
    "# clean 246054\n",
    "# no clean 255431 (considerando tutto anche punteggiatura)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVKaHBR0QPgs"
   },
   "outputs": [],
   "source": [
    "train_keras[[\"item_description_t\",\"name_t\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y40ov-ULHtla"
   },
   "outputs": [],
   "source": [
    "desc_length_max=75\n",
    "inputDesc_train_keras = pad_sequences(train_keras[\"item_description_t\"],\n",
    "                                                  padding='post', maxlen=desc_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tky1YzlkB8U5"
   },
   "outputs": [],
   "source": [
    "inputDesc_validation_keras = pad_sequences(validation_keras[\"item_description_t\"],\n",
    "                                                  padding='post', maxlen=desc_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBZIAP9n3yTU"
   },
   "outputs": [],
   "source": [
    "name_length_max=10\n",
    "inputName_train_keras = pad_sequences(train_keras[\"name_t\"], padding='post', maxlen=name_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uhYXRYQCCHb"
   },
   "outputs": [],
   "source": [
    "inputName_validation_keras = pad_sequences(validation_keras[\"name_t\"], padding='post', maxlen=name_length_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFlm8Qqx-Qab"
   },
   "outputs": [],
   "source": [
    "y_train_keras = train_keras[\"price\"]\n",
    "y_train_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkHBOiRXggJw"
   },
   "outputs": [],
   "source": [
    "y_validation_keras = validation_keras[\"price\"]\n",
    "y_validation_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZh4kWw73yTY"
   },
   "outputs": [],
   "source": [
    "inputA_train_keras = train_keras[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oTri-SwCHqe"
   },
   "outputs": [],
   "source": [
    "inputA_validation_keras = validation_keras[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZmQxJaT3yTX"
   },
   "outputs": [],
   "source": [
    "name_vocab_size= vocab_size\n",
    "desc_vocab_size= vocab_size\n",
    "def getModelKeras(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size):\n",
    "    inputA = Input(shape=(4,))\n",
    "    #Ad = Dense(4, activation='relu')(inputA)\n",
    "    Ad = inputA\n",
    "    \n",
    "    inputName = Input(shape=(name_length_max,))\n",
    "    Ne = Embedding(input_dim=name_vocab_size, output_dim=50, input_length=name_length_max)(inputName)\n",
    "    #Ne = Embedding(\n",
    "    #    num_tokens,\n",
    "    #    embedding_dim,\n",
    "    #    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    #    trainable=False,\n",
    "    #)(inputName)\n",
    "    Nd = Bidirectional(LSTM(12, return_sequences=True, dropout=0.2))(Ne)\n",
    "    Nd =  GlobalMaxPool1D()(Nd)\n",
    "\n",
    "    #Nd = Nf\n",
    "    \n",
    "    inputDesc = Input(shape=(desc_length_max,))\n",
    "    De = Embedding(input_dim=desc_vocab_size, output_dim=50, input_length=desc_length_max)(inputDesc)\n",
    "    #De = Embedding(\n",
    "    #    num_tokens,\n",
    "    #    embedding_dim,\n",
    "    #    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    #    trainable=False,\n",
    "    #)(inputDesc)\n",
    "    Dd = Bidirectional(LSTM(16, return_sequences=True, dropout=0.2))(De)\n",
    "    Dd = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(Dd)\n",
    "    \n",
    "    Dd =  GlobalMaxPool1D()(Dd)\n",
    "    #Dd = Df\n",
    "    \n",
    "    concat = Concatenate()([Ad, Nd, Dd])\n",
    "\n",
    "    x = Dropout(0.2)(concat)    \n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputA, inputName, inputDesc], outputs=x)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWfolHUC3yTX"
   },
   "outputs": [],
   "source": [
    "model_keras = getModelKeras(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size)\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNoRvN6T3yTX"
   },
   "outputs": [],
   "source": [
    "plot_model(model_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-rcb1aLIHPX"
   },
   "outputs": [],
   "source": [
    "model_keras.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQBhACJOZSf"
   },
   "source": [
    "https://machinelearningmastery.com/clean-text-machine-learning-python/ ultima sezione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2m3WcI-JWcP"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "history_keras = model_keras.fit(x=[inputA_train_keras,inputName_train_keras, inputDesc_train_keras], y=y_train_keras,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=([inputA_validation_keras ,inputName_validation_keras, inputDesc_validation_keras], y_validation_keras),\n",
    "                    callbacks=[callback],\n",
    "                    batch_size=512)\n",
    "# senza pulizia  loss: 0.4100  e val_root_mean_squared_logarithmic_error: 0.4543\n",
    "# con pulizia leggera (lower)  val_root_mean_squared_logarithmic_error: 0.455 e loss: 0.4\n",
    "# con pulizia val_root_mean_squared_logarithmic_error: 0.4643 e loss: 0.4269\n",
    "# con lunghezza embedding 50 e senza pulizia val_root_mean_squared_logarithmic_error: 0.4556 e loss: 0.3675 con 10 epoche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6ZqDQxG4de0"
   },
   "outputs": [],
   "source": [
    "histDf_keras = pd.DataFrame(history_keras.history)\n",
    "histDf_keras.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8NNZQXuht7t"
   },
   "outputs": [],
   "source": [
    "histDf_keras.plot(y=[\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgCkkae3Kdmr"
   },
   "outputs": [],
   "source": [
    "inputA_test_keras = test_keras[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')\n",
    "inputName_test_keras = pad_sequences(test_keras[\"name_t\"], padding='post', maxlen=name_length_max)\n",
    "inputDesc_test_keras = pad_sequences(test_keras[\"item_description_t\"], padding='post', maxlen=desc_length_max)\n",
    "pred_keras = model_keras.predict([inputA_test_keras, inputName_test_keras, inputDesc_test_keras])\n",
    "pred_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-IkK1IkNqXD"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeCyj4NcNtuU"
   },
   "outputs": [],
   "source": [
    "validation_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODIe2aySMUZj"
   },
   "outputs": [],
   "source": [
    "pred_keras=pred_keras.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLp_8poJQBmn"
   },
   "outputs": [],
   "source": [
    "pred_v_keras = model.predict([inputA_validation_keras, inputName_validation_keras, inputDesc_validation_keras])\n",
    "pred_v_keras = pred_v_keras.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NmUm_gaRWHL"
   },
   "outputs": [],
   "source": [
    "len(pred_v_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFrRfXG-RhVr"
   },
   "outputs": [],
   "source": [
    "len(y_validation_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJPxtmcQRJlp"
   },
   "outputs": [],
   "source": [
    "root_mean_squared_logarithmic_error(y_validation_keras, pred_v_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G91ZG_SXUU50"
   },
   "source": [
    "# Glove pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNBwsIkTJtuu"
   },
   "source": [
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaYOtsY36rOz"
   },
   "outputs": [],
   "source": [
    "#archive_url = 'http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip'\n",
    "#archive_url = \"https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\"\n",
    "#archive_url = \"https://www.cs.uic.edu/~hxu/ele_review_qa_300d.tar.gz\"\n",
    "archive_url = 'http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.840B.300d.zip'\n",
    "archive_name = 'glove.840B.300d.zip'\n",
    "remove_archive = False\n",
    "embedding_fname = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUeT17cO-xhP"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(archive_name) and not os.path.isfile(embedding_fname):\n",
    "  ! wget {archive_url} -O {archive_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6X2PUTqGNGI"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(embedding_fname):\n",
    "  ! unzip {archive_name} {embedding_fname}\n",
    "\n",
    "# remove archive if already extracted\n",
    "if remove_archive and os.path.isfile(embedding_fname):\n",
    "  os.remove(archive_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTMOU0Pk8A6_"
   },
   "source": [
    "Needs a word encoding index (e.g. keras one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPFLTtl_C6A4"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVnTcD3nCet7"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    base_dir, embedding_fname\n",
    ")\n",
    "\n",
    "'''embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg-i_nmregk7"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYVbr8mGYHqd"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index (padding)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "              i = word_index[word]\n",
    "              try: \n",
    "                embedding_matrix[i] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "              except:\n",
    "                print(word)\n",
    "                print(vector)\n",
    "                print(\"_______________________\")\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HqTmXgE_P8t"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index (padding)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "              i = word_index[word]\n",
    "              try: \n",
    "                embedding_matrix[i] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "                \n",
    "  \n",
    "              except:\n",
    "\n",
    "                vector=vector[1:embedding_dim+1]\n",
    "                embedding_matrix[i] = np.array(vector, dtype=np.float32)\n",
    "\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16SQunrgYXv2"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(path_to_glove_file, tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp2dcxR2fi_n"
   },
   "outputs": [],
   "source": [
    "embedding_matrix[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZBiI8g4eiz8"
   },
   "outputs": [],
   "source": [
    "embedding_matrix[107098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqusQItvZDmS"
   },
   "outputs": [],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size\n",
    "# senza pulizia viene coperto il 28% del vocabolario\n",
    "# con pulizia il 26% senza emoji\n",
    "# con pulizia con emoji 34%\n",
    "# 0.3779 senza pulizia con glove più grande (glove.840B.300d)\n",
    "# 0.467 con pulizia con glove più grande (glove.840B.300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vffbp5FHB-ZN"
   },
   "outputs": [],
   "source": [
    "'''#glove pretrained embedding\n",
    "num_tokens = len(word_index) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It4CwmHQl-zF"
   },
   "source": [
    "**IMPORTANTISSIMO: SCEGLIERE CHE LUNGHEZZA USARE PER L'ECONDING DELLE PAROLE DELLE DESCRIZIONI E DEI NOMI, CIOE' SE 8 VALORI ENTRAMBI O MANTENERE 8 E 16 VALORI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy937m-Yh-3T"
   },
   "outputs": [],
   "source": [
    "name_vocab_size= vocab_size\n",
    "desc_vocab_size= vocab_size\n",
    "def getModelGlove(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size, embedding_matrix=None):\n",
    "    inputA = Input(shape=(4,))\n",
    "    Ad = inputA\n",
    "    \n",
    "    inputName = Input(shape=(name_length_max,))\n",
    "    if embedding_matrix.size == 0:\n",
    "      Ne = Embedding(input_dim=name_vocab_size, output_dim=50, weights=[embedding_matrix], input_length=name_length_max, trainable=False)(inputName)\n",
    "    else: \n",
    "      Ne = Embedding(input_dim=name_vocab_size, output_dim=50, input_length=name_length_max)(inputName)\n",
    "\n",
    "    Nd = Bidirectional(LSTM(12, return_sequences=True, dropout=0.2))(Ne)\n",
    "    Nd =  GlobalMaxPool1D()(Nd)\n",
    "    \n",
    "    inputDesc = Input(shape=(desc_length_max,))\n",
    "    if embedding_matrix.size == 0:\n",
    "      De = Embedding(input_dim=desc_vocab_size, output_dim=50, weights=[embedding_matrix], input_length=desc_length_max, trainable=False)(inputDesc)\n",
    "    else:\n",
    "      De = Embedding(input_dim=desc_vocab_size, output_dim=50, input_length=desc_length_max)(inputDesc)\n",
    "\n",
    "\n",
    "    Dd = Bidirectional(LSTM(16, return_sequences=True, dropout=0.2))(De)\n",
    "    Dd = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(Dd)\n",
    "    \n",
    "    Dd =  GlobalMaxPool1D()(Dd)\n",
    "    \n",
    "    concat = Concatenate()([Ad, Nd, Dd])\n",
    "\n",
    "    x = Dropout(0.2)(concat)    \n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputA, inputName, inputDesc], outputs=x)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh6pXWdkhbzZ"
   },
   "outputs": [],
   "source": [
    "model_glove= getModelGlove(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size, embedding_matrix)\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWVVDyURj6ey"
   },
   "outputs": [],
   "source": [
    "plot_model(model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LeBKCNokAoL"
   },
   "outputs": [],
   "source": [
    "model_glove.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG9QznRskJP-"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "history_glove = model_glove.fit(x=[inputA_train_keras,inputName_train_keras, inputDesc_train_keras], y=y_train_keras,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=([inputA_validation_keras ,inputName_validation_keras, inputDesc_validation_keras], y_validation_keras),\n",
    "                    callbacks=[callback],\n",
    "                    batch_size=512)\n",
    "# glove 6M \n",
    "# con 10 epoche e senza pulizia: loss: 0.5564 e val_root_mean_squared_logarithmic_error: 0.5392\n",
    "# con 10 epoche e pulizia emoji etc: loss: 0.5553 e val_root_mean_squared_logarithmic_error: 0.5312\n",
    "\n",
    "# glove.840B.300d\n",
    "# con 10 epoche senza pulizia: loss: 0.3678  e val_root_mean_squared_logarithmic_error: 0.4537\n",
    "# con 10 epoce con pulizia: loss: 0.3817 e val_root_mean_squared_logarithmic_error 0.4604"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am9eiXhqax3l"
   },
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKpbPcZMddGB"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMvcp3DzpFlE"
   },
   "outputs": [],
   "source": [
    "train_cvec, validation_cvec = train_test_split(data, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc9UaX5XKaRg"
   },
   "outputs": [],
   "source": [
    "categorical_train_cvec= train_cvec[[\"item_condition_id\",\"shipping\",\"category_name_l\",\"brand_name_l\"]]\n",
    "print(\"Train:\")\n",
    "print(categorical_train_cvec.head())\n",
    "print(\"\\nValidation:\")\n",
    "categorical_validation_cvec= validation_cvec[[\"item_condition_id\",\"shipping\",\"category_name_l\",\"brand_name_l\"]]\n",
    "print(categorical_validation_cvec.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I1r2JgCxP1g"
   },
   "outputs": [],
   "source": [
    "y_train_cvec=train_cvec[\"price\"].values\n",
    "y_validation_cvec=validation_cvec[\"price\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuMtKfPMrZBk"
   },
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnLBxl6b0Ruj"
   },
   "outputs": [],
   "source": [
    "vectorizer_desc = CountVectorizer()\n",
    "vectorizer_desc.fit(data[\"item_description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hGHK3_f3du3"
   },
   "outputs": [],
   "source": [
    "len(vectorizer_desc.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvNUFh5zFz09"
   },
   "outputs": [],
   "source": [
    "description_train_cvec = vectorizer_desc.transform(train_cvec[\"item_description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0etpa0ospxMm"
   },
   "outputs": [],
   "source": [
    "description_validation_cvec = vectorizer_desc.transform(validation_cvec[\"item_description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9myswCjJVeB6"
   },
   "outputs": [],
   "source": [
    "type(description_validation_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRsUcwbSpTWu"
   },
   "outputs": [],
   "source": [
    "vectorizer_name = CountVectorizer()\n",
    "vectorizer_name.fit(data[\"name\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx3VVZ15qI0h"
   },
   "outputs": [],
   "source": [
    "len(vectorizer_name.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nh1PddSiqSS_"
   },
   "outputs": [],
   "source": [
    "name_train_cvec = vectorizer_name.transform(train_cvec[\"name\"].values)\n",
    "name_validation_cvec = vectorizer_name.transform(validation_cvec[\"name\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TK01Kzkj6244"
   },
   "outputs": [],
   "source": [
    "type(name_train_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzdkPXSu4VMb"
   },
   "outputs": [],
   "source": [
    "def getModel_bow(categorical_train, name_train, description_train):\n",
    "    inputA = Input(categorical_train.shape[1])\n",
    "    inputName = Input(name_train.shape[1])\n",
    "    inputDesc = Input(description_train.shape[1])\n",
    "    concat = Concatenate()([inputName, inputDesc, inputA])\n",
    "\n",
    "    x = Dropout(0.1)(concat)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputName ,inputDesc, inputA], outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd_anq_Cxg4h"
   },
   "outputs": [],
   "source": [
    "model_cvec = getModel_bow(categorical_train_cvec, name_train_cvec, description_train_cvec)\n",
    "model_cvec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIsxSCOxyzLb"
   },
   "outputs": [],
   "source": [
    "plot_model(model_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93QpCY4F3hXa"
   },
   "outputs": [],
   "source": [
    "model_cvec.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B70Cb3AW7sEw"
   },
   "outputs": [],
   "source": [
    "history_cvec = model_cvec.fit(x=[name_train_cvec, description_train_cvec, categorical_train_cvec.values],\n",
    "                    y=y_train_cvec,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(\n",
    "                        [name_validation_cvec, description_validation_cvec, categorical_validation_cvec.values]\n",
    "                        , y_validation_cvec),\n",
    "                    batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uHhyAV_VXCw"
   },
   "source": [
    "BOW con pulizia:\n",
    "loss: 0.4549 - mse: 877.1694 - mae: 10.7754 - root_mean_squared_error: 29.6144 - mean_squared_logarithmic_error: 0.2073 - root_mean_squared_logarithmic_error: 0.4549 - val_loss: 0.4572 - val_mse: 823.4496 - val_mae: 10.7306 - val_root_mean_squared_error: 28.6958 - val_mean_squared_logarithmic_error: 0.2095 - val_root_mean_squared_logarithmic_error: 0.4573"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbJJkuirlZz1"
   },
   "source": [
    "BOW senza pulizia words:\n",
    "loss: 0.4537 - mse: 892.7444 - mae: 10.7434 - root_mean_squared_error: 29.8743 - mean_squared_logarithmic_error: 0.2063 - root_mean_squared_logarithmic_error: 0.4537 - val_loss: 0.4554 - val_mse: 848.4371 - val_mae: 10.7117 - val_root_mean_squared_error: 29.1279 - val_mean_squared_logarithmic_error: 0.2079 - val_root_mean_squared_logarithmic_error: 0.4555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf88pKGe_6FI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    rmsle = history.history['root_mean_squared_logarithmic_error']\n",
    "    val_rmsle = history.history['val_root_mean_squared_logarithmic_error']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(rmsle) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, rmsle, 'b', label='Training rmsle')\n",
    "    plt.plot(x, val_rmsle, 'r', label='Validation rmsle')\n",
    "    plt.title('Training and validation root_mean_squared_logarithmic_error')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgzFRzm-__Se"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uusyBK_m75L"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1DFlMbsm95Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0qBBoukwa_K"
   },
   "outputs": [],
   "source": [
    "data[\"item_description\"] = data[\"item_description\"]\n",
    "data[\"name\"] = data[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHvnuhG_nF-o"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_desc = TfidfVectorizer()\n",
    "tfidf_vectorizer_desc.fit(data[\"item_description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNsnLmuEnOCA"
   },
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer_desc.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FEV_JeYnOq7"
   },
   "outputs": [],
   "source": [
    "tfidf_description_train = tfidf_vectorizer_desc.transform(train_cvec[\"item_description\"].values)\n",
    "tfidf_description_validation = tfidf_vectorizer_desc.transform(validation_cvec[\"item_description\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-3628USnioQ"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_name = TfidfVectorizer()\n",
    "tfidf_vectorizer_name.fit(data[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-qOh3FXnzSb"
   },
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer_name.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOnnouCxn_he"
   },
   "outputs": [],
   "source": [
    "tfidf_name_train = tfidf_vectorizer_name.transform(train_cvec[\"name\"])\n",
    "tfidf_name_validation = tfidf_vectorizer_name.transform(validation_cvec[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCSrpsML5e47"
   },
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO23VoG_Umkx"
   },
   "outputs": [],
   "source": [
    "type(tfidf_name_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyGnExtwqcgx"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/61961042/indices201-0-8-is-out-of-order-many-sparse-ops-require-sorted-indices-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHw1A89wXWw0"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "new_categorical_train_cvec=scipy.sparse.csr_matrix(categorical_train_cvec.values)\n",
    "new_categorical_validation_cvec=scipy.sparse.csr_matrix(categorical_validation_cvec.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeLdc-Esipcg"
   },
   "outputs": [],
   "source": [
    "new_tfidf_categorical_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_categorical_validation_cvec))\n",
    "new_tfidf_categorical_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_categorical_train_cvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsRkzDbNhVPN"
   },
   "outputs": [],
   "source": [
    "new_tfidf_name_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_name_validation))\n",
    "new_tfidf_name_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_name_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tXBiPsZiE19"
   },
   "outputs": [],
   "source": [
    "type(new_tfidf_name_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4XHrIyGiHS_"
   },
   "outputs": [],
   "source": [
    "type(tfidf_name_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqX5vyv5jMjJ"
   },
   "outputs": [],
   "source": [
    "new_tfidf_description_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_description_validation))\n",
    "new_tfidf_description_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_description_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rBPl6RDX7qM"
   },
   "outputs": [],
   "source": [
    "'''new_y_train_cvec=scipy.sparse.csr_matrix(y_train_cvec)\n",
    "new_y_validation_cvec=scipy.sparse.csr_matrix(y_validation_cvec)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOJzQCvZi1MH"
   },
   "outputs": [],
   "source": [
    "'''new_tfidf_y_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_validation_cvec))\n",
    "new_tfidf_y_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_train_cvec))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBQK0C3bWkQs"
   },
   "outputs": [],
   "source": [
    "'''type(tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_train_cvec)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKQCzVxn9GT1"
   },
   "outputs": [],
   "source": [
    "def getModel_bow(name_train, description_train, categorical_train):\n",
    "    inputA = Input(categorical_train.shape[1])\n",
    "    inputName = Input(name_train.shape[1])\n",
    "    inputDesc = Input(description_train.shape[1])\n",
    "    concat = Concatenate()([inputName, inputDesc, inputA])\n",
    "\n",
    "    #x = Dropout(0.1)(concat)\n",
    "    x = Dense(32, activation='relu')(concat)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputName ,inputDesc, inputA], outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XK-E2e0oKeC"
   },
   "outputs": [],
   "source": [
    "model = getModel_bow(new_tfidf_name_train, new_tfidf_description_train, categorical_validation_cvec)\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqJ7pgfosi4q"
   },
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4Fv4q35ocWN"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a80wIgkOoelZ"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[new_tfidf_name_train, new_tfidf_description_train, new_tfidf_categorical_train], y=y_train_cvec,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=([new_tfidf_name_validation, new_tfidf_description_validation, new_tfidf_categorical_validation], y_validation_cvec),\n",
    "                    batch_size=512)\n",
    "# 0.4771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RBlRb_gujSz"
   },
   "outputs": [],
   "source": [
    "history2 = model.fit(x=[new_tfidf_name_train, new_tfidf_description_train], y=y_train_cvec,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=([new_tfidf_name_validation, new_tfidf_description_validation], y_validation_cvec),\n",
    "                    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3QDoGQvxpCl"
   },
   "outputs": [],
   "source": [
    "histdf = pd.DataFrame(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH8wryVvxtaA"
   },
   "outputs": [],
   "source": [
    "histdf.plot(y=[\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIrZwW2wq7r7"
   },
   "source": [
    "tf-idf gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPe1LMPVq9ln"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA-0p3DMritP"
   },
   "outputs": [],
   "source": [
    "data[\"item_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CO2a-oguycH"
   },
   "outputs": [],
   "source": [
    "doc_tokenized = [simple_preprocess(doc) for doc in data[\"item_description\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDjGjel3vHhk"
   },
   "outputs": [],
   "source": [
    "doc_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8L2nbDrvQly"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gBzeihTvVom"
   },
   "outputs": [],
   "source": [
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LafgQkjqv4dV"
   },
   "outputs": [],
   "source": [
    "# stampa le parole che compaiono in ogni descrizione con la loro frequenza di apparizione\n",
    "for doc in BoW_corpus[:10]:\n",
    "   print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYA8xxQOyabr"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VjEh4WP0loV"
   },
   "outputs": [],
   "source": [
    "for doc in tfidf[BoW_corpus]:\n",
    "   print([[dictionary[id], np.around(freq)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tFxY7se1Uyo"
   },
   "outputs": [],
   "source": [
    "data[\"tfidf\"]=tfidf[BoW_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCbOqhV03DIX"
   },
   "outputs": [],
   "source": [
    "len(data.iloc[10][\"tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZ0CKj8s30wY"
   },
   "outputs": [],
   "source": [
    "train_tfidf, validation_tfidf = train_test_split(data[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\", \"tfidf\", \"price\"]], test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aG4lyxj50R5"
   },
   "outputs": [],
   "source": [
    "train_tfidf_categorical=train_tfidf[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\"]]\n",
    "validation_tfidf_categorical=validation_tfidf[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\"]]\n",
    "\n",
    "\n",
    "train_tfidf_description=train_tfidf[\"tfidf\"]\n",
    "validation_tfidf_description=validation_tfidf[\"tfidf\"]\n",
    "\n",
    "y_train=train_tfidf[\"price\"]\n",
    "y_validation=validation_tfidf[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_507TTwr6rmz"
   },
   "outputs": [],
   "source": [
    "(validation_tfidf_description[0:10].todense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCIuh8-24ied"
   },
   "outputs": [],
   "source": [
    "def getModel_bow(categorical_train, description_train):\n",
    "    inputA = Input(categorical_train.shape[1])\n",
    "    inputDesc = Input(description_train.shape[1])\n",
    "    concat = Concatenate()([inputDesc, inputA])\n",
    "\n",
    "    x = Dropout(0.1)(concat)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[inputDesc, inputA], outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUUxfAFd4xGk"
   },
   "outputs": [],
   "source": [
    "model = getModel_bow(train_tfidf_categorical ,train_tfidf_description)\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3YG-zBS7-mp"
   },
   "outputs": [],
   "source": [
    "train_tfidf_description.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE4hyzsKBQRu"
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLeogNaNBe6J"
   },
   "outputs": [],
   "source": [
    "pret_model_trans = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "pret_model_trans.trainable = False\n",
    "\n",
    "tokenizer_trans = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wX_KoNI_1k4G"
   },
   "outputs": [],
   "source": [
    "num_batches = 100\n",
    "\n",
    "trans_batch_starti = list(range(0, data.shape[0], int(data.shape[0] / num_batches)))[0:num_batches]\n",
    "trans_batch_starti.append(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-9cTcBV19U0"
   },
   "outputs": [],
   "source": [
    "trans_batches = [(trans_batch_starti[i], trans_batch_starti[i+1]) for i in range(0,len(trans_batch_starti)-1)]\n",
    "[(i,b) for i,b in enumerate(trans_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmjfqcS310JG"
   },
   "outputs": [],
   "source": [
    "# divide in batch as we don't have enough memory to handle all at once\n",
    "# 0 to 9\n",
    "trans_batch_number = 0\n",
    "\n",
    "trans_batch = trans_batches[trans_batch_number]\n",
    "trans_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgUJwyC2pigv"
   },
   "outputs": [],
   "source": [
    "print(\"Total shape\", data.shape)\n",
    "data_batch = data[trans_batch[0]:trans_batch[1]]\n",
    "print(\"Batch shape\", data_batch.shape)\n",
    "\n",
    "train_trans, validation_trans = train_test_split(data_batch, test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMQCPY-sSjDy"
   },
   "outputs": [],
   "source": [
    "y_train_trans = train_trans[\"price\"]\n",
    "y_train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmlNBK-aSn0L"
   },
   "outputs": [],
   "source": [
    "y_validation_trans = validation_trans[\"price\"]\n",
    "y_validation_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpeCVE5xBJnx"
   },
   "source": [
    "todo: use cleaned? or let bert handle everything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZnPt2pwPhda"
   },
   "outputs": [],
   "source": [
    "inputA_train_trans = train_trans[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1vQL1C1Pchc"
   },
   "outputs": [],
   "source": [
    "inputA_validation_trans = validation_trans[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEQjK8h4l0mg"
   },
   "outputs": [],
   "source": [
    "trans_name_tokenizer_maxlength = 20\n",
    "trans_desc_tokenizer_maxlength = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEeOZukjBF3i"
   },
   "outputs": [],
   "source": [
    "inputName_train_trans = tokenizer_trans(train_trans[\"name\"].to_list(),\n",
    "                                  return_tensors=\"tf\",\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length = trans_name_tokenizer_maxlength)\n",
    "inputName_train_trans['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ8sh5pUPrIC"
   },
   "outputs": [],
   "source": [
    "inputName_validation_trans = tokenizer_trans(validation_trans[\"name\"].to_list(),\n",
    "                                  return_tensors=\"tf\",\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length = trans_name_tokenizer_maxlength)\n",
    "inputName_validation_trans['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bQnsc9-JcIq"
   },
   "outputs": [],
   "source": [
    "inputName_shape_trans = (inputName_train_trans['input_ids'].shape[1],\n",
    "                         inputName_train_trans['attention_mask'].shape[1])\n",
    "\n",
    "inputName_shape_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMjIqkOs2jhU"
   },
   "outputs": [],
   "source": [
    "inputDesc_train_trans = tokenizer_trans(train_trans[\"item_description_clean\"].to_list(),\n",
    "                                  return_tensors=\"tf\",\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=trans_desc_tokenizer_maxlength)\n",
    "inputDesc_train_trans['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZccJ_0HUP5EX"
   },
   "outputs": [],
   "source": [
    "inputDesc_validation_trans = tokenizer_trans(validation_trans[\"item_description_clean\"].to_list(),\n",
    "                                  return_tensors=\"tf\",\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=trans_desc_tokenizer_maxlength)\n",
    "inputDesc_validation_trans['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCTEIDnSJsMP"
   },
   "outputs": [],
   "source": [
    "inputDesc_shape_trans = (inputDesc_train_trans['input_ids'].shape[1],\n",
    "                         inputDesc_train_trans['attention_mask'].shape[1])\n",
    "inputDesc_shape_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_odyHkIAJWy"
   },
   "outputs": [],
   "source": [
    "def getModel_trans():\n",
    "    inputA = Input(shape=(4,))\n",
    "    Ad = inputA    \n",
    "    \n",
    "    inputName_ids = Input(shape=(inputName_shape_trans[0],), dtype='int32')\n",
    "    inputName_mask = Input(shape=(inputName_shape_trans[1],), dtype='int32')\n",
    "\n",
    "    Np = pret_model_trans(inputName_ids, attention_mask=inputName_mask)[0]\n",
    "\n",
    "    Nd = GlobalMaxPool1D()(Np)\n",
    "\n",
    "    inputDesc_ids = Input(shape=(inputDesc_shape_trans[0],), dtype='int32')\n",
    "    inputDesc_mask = Input(shape=(inputDesc_shape_trans[1],), dtype='int32')\n",
    "\n",
    "    Dp = pret_model_trans(inputDesc_ids, attention_mask=inputDesc_mask)[0]\n",
    "\n",
    "    Dd = GlobalMaxPool1D()(Dp)\n",
    "\n",
    "    concat = Concatenate()([Ad, Nd, Dd])\n",
    "\n",
    "    x = Dropout(0.1)(concat)    \n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=[\n",
    "                          inputA,\n",
    "                          inputName_ids,\n",
    "                          inputName_mask,\n",
    "                          inputDesc_ids,\n",
    "                          inputDesc_mask\n",
    "                          ], outputs=x)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN9opFTBHLXu"
   },
   "outputs": [],
   "source": [
    "model_trans = getModel_trans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdOujyB1HcZ3"
   },
   "outputs": [],
   "source": [
    "model_trans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e00K_QQ8O6Nq"
   },
   "outputs": [],
   "source": [
    "plot_model(model_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyqut5HjPBR4"
   },
   "outputs": [],
   "source": [
    "model_trans.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV6080wZPLlo"
   },
   "outputs": [],
   "source": [
    "history_trans = model_trans.fit(x=[inputA_train_trans,\n",
    "                   inputName_train_trans['input_ids'],\n",
    "                   inputName_train_trans['attention_mask'],\n",
    "                   inputDesc_train_trans['input_ids'],\n",
    "                   inputDesc_train_trans['attention_mask']\n",
    "                  ],\n",
    "                   y=y_train_trans,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=([\n",
    "                                      inputA_validation_trans,\n",
    "                                      inputName_validation_trans['input_ids'],\n",
    "                                      inputName_validation_trans['attention_mask'],\n",
    "                                      inputDesc_validation_trans['input_ids'],\n",
    "                                      inputDesc_validation_trans['attention_mask'],\n",
    "                                      ], \n",
    "                                     y_validation_trans),\n",
    "                    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6XYsUVDDtta"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cw5N2UjxiPcW",
    "4m7ykQvijKp8",
    "WE4hyzsKBQRu"
   ],
   "name": "MercariPrice1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
