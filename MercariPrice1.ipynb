{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MercariPrice1_new.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Jl7czt1455gD",
        "Kdorusqa1sBT",
        "G91ZG_SXUU50",
        "Am9eiXhqax3l",
        "uuMtKfPMrZBk",
        "1uusyBK_m75L",
        "WE4hyzsKBQRu"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_oO4ZyYU00Y"
      },
      "source": [
        "### Mercari Price \n",
        "The files consist of a list of product listings. These files are tab-delimited.\n",
        "\n",
        "Fields:\n",
        "- train_id or test_id - the id of the listing\n",
        "\n",
        "- name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid  leakage. These removed prices are represented as [rm]\n",
        "\n",
        "- item_condition_id - the condition of the items provided by the seller\n",
        "\n",
        "- category_name - category of the listing\n",
        "\n",
        "- brand_name\n",
        "\n",
        "- price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.\n",
        "\n",
        "- shipping - 1 if shipping fee is paid by seller and 0 by buyer\n",
        "\n",
        "- item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. $20) to avoid leakage. These removed prices are represented as [rm]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTBm7hc-CoRA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZWiRNuUU00w"
      },
      "source": [
        "! pip install pydot graphviz emoji transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input, Embedding, Concatenate, Flatten, Dropout, LSTM, GlobalMaxPool1D, GRU, Bidirectional, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import emoji\n",
        "import os\n",
        "\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertModel, pipeline\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove(\"no\")\n",
        "\n",
        "tqdm_notebook.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ETrpfjg_ha"
      },
      "source": [
        "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "\n",
        "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
        "    return K.sqrt(msle(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRb9LhAxe5X"
      },
      "source": [
        "# seed\n",
        "random_seed = 1000\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRXV0oKS-Yn3"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckhXGgT8-XzG"
      },
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# check if in colab\n",
        "if RunningInCOLAB and not os.path.isdir('/content/gdrive'):\n",
        "    print(\"Running in colab\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    colab_root = '/content/drive'\n",
        "      \n",
        "if RunningInCOLAB:\n",
        "    root_dir = \"/content/gdrive/My Drive/\"\n",
        "    base_dir = root_dir + 'project-mercari-price/'\n",
        "    if not os.path.isdir(base_dir):\n",
        "        os.mkdir(base_dir)\n",
        "else:\n",
        "    root_dir= os.getcwd()\n",
        "    base_dir = root_dir\n",
        "\n",
        "os.chdir(base_dir)\n",
        "\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMt4O6n3yTD"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXCUW4PtU00k"
      },
      "source": [
        "dataset_downloaded_path = os.path.join(base_dir, \"dataset_downloaded.ignore\")\n",
        "dataset_downloaded = os.path.isfile(dataset_downloaded_path)\n",
        "dataset_downloaded\n",
        "\n",
        "if not dataset_downloaded:\n",
        "  # install kaggle to download dataset\n",
        "  ! pip install kaggle python-dotenv\n",
        "\n",
        "# set to True if you want to save kaggle credentials into a .env file\n",
        "persist_credentials = False\n",
        "\n",
        "if not dataset_downloaded:\n",
        "  # create .env file containing KAGGLE_USER and KAGGLE_KEY\n",
        "    kaggle_env = os.path.join(base_dir, '.env')\n",
        "    if not os.path.isfile(kaggle_env):\n",
        "        with open(kaggle_env, 'w') as envfile:\n",
        "            kaggle_user = input(\"Insert kaggle username\")\n",
        "            kaggle_key = input(\"Insert kaggle key; generate one from kaggle account\")\n",
        "        if persist_credentials:\n",
        "            envfile.write(f\"\"\"\n",
        "            KAGGLE_USERNAME={kaggle_user}\n",
        "            KAGGLE_KEY={kaggle_key}\n",
        "            \"\"\")\n",
        "\n",
        "        # set env vars\n",
        "        os.environ[\"KAGGLE_USERNAME\"] = kaggle_user\n",
        "        os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
        "\n",
        "        del kaggle_user\n",
        "        del kaggle_key\n",
        "\n",
        "if not dataset_downloaded:\n",
        "  # loading env vars if .env file exists\n",
        "    if os.path.isfile(kaggle_env):\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv(dotenv_path=kaggle_env)\n",
        "    print(os.environ.get(\"KAGGLE_USERNAME\"))\n",
        "\n",
        "if not dataset_downloaded:\n",
        "    # download and extract dataset\n",
        "    ! kaggle competitions download -c mercari-price-suggestion-challenge\n",
        "\n",
        "    # create file so that we know we already downloaded\n",
        "    with open(dataset_downloaded_path, 'w') as dd_file:\n",
        "        dataset_downloaded = True\n",
        "        dd_file.write(\"\")\n",
        "\n",
        "    print('cwd: ', os.getcwd())\n",
        "    \n",
        "    os.listdir()\n",
        "\n",
        "if not dataset_downloaded:\n",
        "    ! 7z x train.tsv.7z\n",
        "    ! 7z x test.tsv.7z\n",
        "\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pyEueRK3yTL"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Vg17INU00x"
      },
      "source": [
        "dtypes={\n",
        "    'name': 'string',\n",
        "    'item_condition_id': 'int32',\n",
        "    'category_name': 'string',\n",
        "    'brand_name': 'string',\n",
        "    'price': 'float',\n",
        "    'shipping': 'int32',\n",
        "    'item_description': 'string'\n",
        "}\n",
        "data = pd.read_csv(\"train.tsv\", sep='\\t', dtype=dtypes)\n",
        "data = data.drop(columns=[\"train_id\"])\n",
        "print(data.dtypes)\n",
        "print(data.shape)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWgi5SxW9SsE"
      },
      "source": [
        "dtypes={\n",
        "    'name': 'string',\n",
        "    'item_condition_id': 'int32',\n",
        "    'category_name': 'string',\n",
        "    'brand_name': 'string',\n",
        "    'price': 'float',\n",
        "    'shipping': 'int32',\n",
        "    'item_description': 'string'\n",
        "}\n",
        "test = pd.read_csv(\"test.tsv\", sep='\\t', dtype=dtypes)\n",
        "test = test.drop(columns=[\"test_id\"])\n",
        "print(test.dtypes)\n",
        "print(test.shape)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvujMFFXCxL8"
      },
      "source": [
        "for column in data.columns:\n",
        "    print(\"number of null value in {} : {}\".format(column,data[column].isnull().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NltZr2wAdYc"
      },
      "source": [
        "for column in test.columns:\n",
        "    print(\"number of null value in {} : {}\".format(column,test[column].isnull().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOYErCoVHnmO"
      },
      "source": [
        "data[\"price\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM7PCRQM3Jn9"
      },
      "source": [
        "import plotly.express as px\r\n",
        "fig = px.histogram(data, x=\"price\",)\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQylMKwj3Lum"
      },
      "source": [
        "data[\"shipping\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n06t6AB3NR-"
      },
      "source": [
        "data[\"shipping\"].value_counts()[0]/len(data[\"shipping\"])*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1YYbAdS3O2P"
      },
      "source": [
        "data[data[\"shipping\"]==0][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQU6zPBt3Q0Y"
      },
      "source": [
        "data[data[\"shipping\"]==1][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4iucupe3TPg"
      },
      "source": [
        "fig = px.histogram(data, x=\"price\", color=\"shipping\")\r\n",
        "fig.update_layout(xaxis_type=\"log\", yaxis_type=\"log\")\r\n",
        "\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxgmC-LHnmN"
      },
      "source": [
        "data[\"category_name\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuuI6Skq6HhN"
      },
      "source": [
        "data[\"item_condition_id\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dU9kQqp6Kxy"
      },
      "source": [
        "data[data[\"item_condition_id\"]==1][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK_cyfJS6OFz"
      },
      "source": [
        "data[data[\"item_condition_id\"]==2][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb394yH86TF7"
      },
      "source": [
        "data[data[\"item_condition_id\"]==3][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vMMnrBO6WBL"
      },
      "source": [
        "data[data[\"item_condition_id\"]==4][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECkOm0Bc6Xb0"
      },
      "source": [
        "data[data[\"item_condition_id\"]==5][\"price\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD4xUAnh6Yvv"
      },
      "source": [
        "def split_cat(text):\r\n",
        "    try: return text.split(\"/\")\r\n",
        "    except: return (\"No Label\", \"No Label\", \"No Label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HX47KAH6eiT"
      },
      "source": [
        "data['general_cat'], data['subcat_1'], data['subcat_2'] = \\\r\n",
        "zip(*data['category_name'].apply(lambda x: split_cat(x)))\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFz0wIBV6gv9"
      },
      "source": [
        "data[\"brand_name\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAUIFZQ46hjJ"
      },
      "source": [
        "len(data[data[\"item_description\"].str.lower() == \"no description yet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idKBJgLb6kkU"
      },
      "source": [
        "data[\"len_desc\"]=data['item_description'].str.len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_fdM4nH6lht"
      },
      "source": [
        "data[[\"price\",\"len_desc\"]].corr(method='pearson')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y96fEbW6wu9"
      },
      "source": [
        "uno=data[data[\"price\"]>=100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPruy4NE63vT"
      },
      "source": [
        "due=data[(data[\"price\"]>50) & (data[\"price\"]<100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbeLRDnD7GFD"
      },
      "source": [
        "tre=data[(data[\"price\"]>30) & (data[\"price\"]<=50)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctL3KVPj7IYr"
      },
      "source": [
        "quattro=data[data[\"price\"]<=30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk3ShJ2n7MXm"
      },
      "source": [
        "def flat_list(l):\r\n",
        "    return  [item for sublist in l for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab5_h3RY7Ntq"
      },
      "source": [
        "def word_Cloud(sentences):\r\n",
        "    flat_sentences = flat_list(sentences)\r\n",
        "    print(flat)\r\n",
        "    counter = Counter(flat_sentences)\r\n",
        "    cdict = dict(counter.most_common())\r\n",
        "\r\n",
        "    wcloud = wordcloud.WordCloud(background_color=\"white\").generate_from_frequencies(cdict)\r\n",
        "    plt.figure(figsize = (10, 8), facecolor = None) \r\n",
        "    plt.imshow(wcloud, interpolation='bilinear')\r\n",
        "    plt.axis(\"off\")\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxBs1Xr4AlsD"
      },
      "source": [
        "data[\"item_description\"] = data[\"item_description\"].fillna(value=\"NA\")\n",
        "data[\"brand_name\"] = data[\"brand_name\"].fillna(value=\"NA\")\n",
        "data[\"category_name\"] = data[\"category_name\"].fillna(value=\"NA\")\n",
        "# see warnings -> inplace?\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOMIwETU7Ock"
      },
      "source": [
        "uno[\"bigrams_description\"]=(uno[\"item_description\"].apply(text_to_word_sequence)).progress_apply(lambda row: list(nltk.ngrams(row, 2)))\r\n",
        "uno[\"bigrams_description\"]= uno[\"bigrams_description\"].progress_apply(lambda tupla: [' '.join(elem) for elem in tupla])\r\n",
        "word_Cloud(uno[\"bigrams_description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLnzNyQj7P--"
      },
      "source": [
        "due[\"bigrams_description\"]=(due[\"item_description\"].apply(text_to_word_sequence)).progress_apply(lambda row: list(nltk.ngrams(row, 2)))\r\n",
        "due[\"bigrams_description\"]= due[\"bigrams_description\"].progress_apply(lambda tupla: [' '.join(elem) for elem in tupla])\r\n",
        "word_Cloud(due[\"bigrams_description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9PXRo9b7R5F"
      },
      "source": [
        "tre[\"bigrams_description\"]=(tre[\"item_description\"].apply(text_to_word_sequence)).progress_apply(lambda row: list(nltk.ngrams(row, 2)))\r\n",
        "tre[\"bigrams_description\"]= tre[\"bigrams_description\"].progress_apply(lambda tupla: [' '.join(elem) for elem in tupla])\r\n",
        "word_Cloud(tre[\"bigrams_description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aod-qanW7TfY"
      },
      "source": [
        "quattro[\"bigrams_description\"]=(quattro[\"item_description\"].apply(text_to_word_sequence)).progress_apply(lambda row: list(nltk.ngrams(row, 2)))\r\n",
        "quattro[\"bigrams_description\"]= quattro[\"bigrams_description\"].progress_apply(lambda tupla: [' '.join(elem) for elem in tupla])\r\n",
        "word_Cloud(quattro[\"bigrams_description\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCNw-9g37VN1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5N2UjxiPcW"
      },
      "source": [
        "# Data cleaning\n",
        "\n",
        "Handle missing values and wrong prices.\n",
        "\n",
        "Prices should be in the range [5,2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnzdF-qLrlGI"
      },
      "source": [
        "https://www.mercari.com/us/help_center/article/69"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyUE4a4Jp4E9"
      },
      "source": [
        "len(data[(data[\"price\"]<5) | (data[\"price\"]>2000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCbqNKgKp7f2"
      },
      "source": [
        "data=data[(data[\"price\"]>=5) & (data[\"price\"]<=2000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUBsIEJqC0cg"
      },
      "source": [
        "test[\"brand_name\"] = test[\"brand_name\"].fillna(value=\"NA\")\n",
        "test[\"category_name\"] = test[\"category_name\"].fillna(value=\"NA\")\n",
        "# see warnings -> inplace?\n",
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNY29qP2HnmS"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m7ykQvijKp8"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkGhq22YJJGf"
      },
      "source": [
        "data[\"item_description\"]=data[\"item_description\"].str.lower()\n",
        "data[\"name\"]=data[\"name\"].str.lower()\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRMYM8kLpH3e"
      },
      "source": [
        "data[\"item_description\"]=data[\"item_description\"].replace(\"no description yet\", \"NA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oRa29_YxI4G"
      },
      "source": [
        "## Text cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WynSk_Mdj44"
      },
      "source": [
        "tweetTokenizer = TweetTokenizer()\n",
        "\n",
        "def list_to_str(l):\n",
        "   return ' '.join([str(elem) for elem in l])\n",
        "   \n",
        "def textCleanup(df, flag=True):\n",
        "  df=df.to_frame(name=\"str\")\n",
        "  #df[\"clean\"] = df[\"str\"].progress_apply(text_to_word_sequence)   # 20 secondi\n",
        "  global tweetTokenizer\n",
        "  df[\"clean\"] = df[\"str\"].progress_apply(tweetTokenizer.tokenize) # 2 minutes but correctly handles emojis\n",
        "\n",
        "  # punct and stop words\n",
        "  \n",
        "\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  \n",
        "  df[\"clean\"] = df[\"clean\"].progress_apply(lambda sentence : [lemmatizer.lemmatize(word) for word in sentence if word not in stop_words]) # 10 secondi\n",
        "  df[\"clean\"] = df[\"clean\"].progress_apply(lambda sentence:\n",
        "                                           [w for w in sentence if w\n",
        "                                              not in string.punctuation\n",
        "                                              and w not in stop_words and (len(w)>1 or w.isdigit()) and w not in emoji.UNICODE_EMOJI]) # 18 s\n",
        "  plot_common_tokens(df[\"clean\"], \"Most Common Tokens without StopWords\", n=20)\n",
        "  if flag:\n",
        "    df[\"clean\"] = df[\"clean\"].progress_apply(list_to_str) # 6 secondi\n",
        "  return df[\"clean\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwTCeUoL6Rq7"
      },
      "source": [
        "def preprocessData(data):\n",
        "  print('description clean up')\n",
        "  data[\"item_description_clean\"] = textCleanup(data[\"item_description\"]) \n",
        "\n",
        "  print('name clean up')\n",
        "  data[\"name_clean\"] = textCleanup(data[\"name\"])\n",
        "  \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JELRL6ipo0D"
      },
      "source": [
        "def flat_list(l):\n",
        "    return  [item for sublist in l for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMSrHLKdpWQL"
      },
      "source": [
        "def plot_common_tokens(tokens, title, n=20):\n",
        "    sentences = (list(itertools.chain(tokens)))\n",
        "    flat_sentences = flat_list(sentences)\n",
        "    counts = Counter(flat_sentences)\n",
        "    #print(counts.most_common(30))\n",
        "    common_words = [word[0] for word in counts.most_common(n)]\n",
        "    common_counts = [word[1] for word in counts.most_common(n)]\n",
        "    fig = plt.figure(figsize=(18,6))\n",
        "    sns.barplot(x=common_words, y=common_counts)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TZbvig1qHceY"
      },
      "source": [
        "data = preprocessData(data)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhb4O3Atw70"
      },
      "source": [
        "test = preprocessData(test)\r\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Gt-WCFxEwH"
      },
      "source": [
        "## Categories split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WLvkbN8uf-v"
      },
      "source": [
        "Most category_name (~99.7%) contains 3 subcategories or less.\n",
        "\n",
        "We consider the first 3 subcategories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knF_RFEatvcY"
      },
      "source": [
        "data[\"category_name\"].apply(lambda x: x.count('/')+1).value_counts().sort_index() / data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuz6NYQTRgzE"
      },
      "source": [
        "def split_cat(text):\n",
        "  \"\"\"\n",
        "  function to split category into 3 subcategories\n",
        "\n",
        "  e.g:\n",
        "  Men/Tops/T-shirts -> [\"Men\", \"Tops\", \"T-shirts\"]\n",
        "\n",
        "  in case of more than 3 subcategories (~0.3% of the dataset)\n",
        "  the last categories will be bounded toghether e.g.\n",
        "  \n",
        "  Men/Tops/T-shirts/RedTshirts/FlameTshirts ->\n",
        "  -> [\"Men\", \"Tops\", \"T-shirts/RedTshirts/FlameTshirts\"]\n",
        "\n",
        "  in case of less than 3 if fills with 'NA'\n",
        "  \"\"\"\n",
        "  r = text.split(\"/\", 2)\n",
        "  while len(r) < 3:\n",
        "    r = r + ['NA']\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MnnbHc9RUVy"
      },
      "source": [
        "data['cat1'], data['cat2'], data['cat3'] = zip(*data['category_name'].progress_apply(lambda x: split_cat(x)))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkr1Ueo7w3ws"
      },
      "source": [
        "# Test preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-V9ozHxNrD"
      },
      "source": [
        "## Text cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqevrhyRC7yy"
      },
      "source": [
        "test[\"item_description\"]=test[\"item_description\"].str.lower()\n",
        "test[\"name\"]=test[\"name\"].str.lower()\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE3PL_bmDNjK"
      },
      "source": [
        "test = preprocessData(test)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sf4BfkfxQMD"
      },
      "source": [
        "## Categories split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN6uzYB_w90e"
      },
      "source": [
        "test['cat1'], test['cat2'], test['cat3'] = zip(*test['category_name'].progress_apply(lambda x: split_cat(x)))\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC4xOHj_0zow"
      },
      "source": [
        "# Load preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b38isOpV0ejM"
      },
      "source": [
        "fname = 'train_preprocess.npy'\n",
        "if 'data' in globals():\n",
        "  # update preprocessed data\n",
        "  print(\"saving preprocess train data\")\n",
        "  data.to_pickle(fname)\n",
        "else:\n",
        "  print(\"loading existing preprocess train data\")\n",
        "  data = pd.read_pickle(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LJ_alCc1znc"
      },
      "source": [
        "fname = 'test_preprocess.npy'\n",
        "\n",
        "if 'test' in globals():\n",
        "  # update preprocess test data\n",
        "  print(\"saving preprocess test data\")\n",
        "  test.to_pickle(fname)\n",
        "else:\n",
        "  print(\"loading existing preprocess test data\")\n",
        "  test = pd.read_pickle(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GdqYoopwUca"
      },
      "source": [
        "# Train Validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mbI-4SRwXpf"
      },
      "source": [
        "train, validation = train_test_split(data, test_size=0.2, random_state=random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYZRBnbN0sv-"
      },
      "source": [
        "# Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trBPru1qEK5b"
      },
      "source": [
        "# TODO ensure it is correct or use a well tested alternative like sklearn (found problems with dimensions)\n",
        "class LabelEncoder:\n",
        "  \"\"\"\n",
        "  Simple single dimension label encoder class able to handle\n",
        "  unknown values \n",
        "  \"\"\"\n",
        "  def __init__(self, unknown = 0, invUnknown = 'unknown'):\n",
        "    self.leDict = {}\n",
        "    self.invDict = {}\n",
        "    self.unknown = unknown\n",
        "    self.invUnknown = invUnknown\n",
        "\n",
        "  def fit(self, data):\n",
        "    vci = pd.value_counts(data).index\n",
        "    self.leDict = dict(zip(vci, range(1, len(vci)+1)))\n",
        "    self.invDict = dict(zip(range(1, len(vci)+1), vci))\n",
        "\n",
        "  def transform1(self, item):\n",
        "    return self.leDict.get(item, self.unknown)\n",
        "\n",
        "  def transform(self, data):\n",
        "    return data.apply(lambda item: self.transform1(item))\n",
        "\n",
        "  def inverse_transform1(self, item):\n",
        "    return self.invDict.get(item, self.invUnknown)\n",
        "\n",
        "  def inverse_transform(self, data):\n",
        "    return data.apply(lambda item: self.inverse_transform1(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO4EL__E3yTS"
      },
      "source": [
        "cat_le = LabelEncoder()\n",
        "cat_le.fit(np.hstack([train[\"cat1\"], train['cat2'], train['cat3']]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JShvIXjWyf-_"
      },
      "source": [
        "train[\"cat1_l\"] = cat_le.transform(train[\"cat1\"])\n",
        "train[\"cat2_l\"] = cat_le.transform(train[\"cat2\"])\n",
        "train[\"cat3_l\"] = cat_le.transform(train[\"cat3\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfiZte2s3yTS"
      },
      "source": [
        "brand_le = LabelEncoder()\n",
        "brand_le.fit(train[\"brand_name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fRbStz9yiM1"
      },
      "source": [
        "train[\"brand_name_l\"] = brand_le.transform(train[\"brand_name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVxAfaLByyb6"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v7aDe4Hy4qh"
      },
      "source": [
        "validation[\"cat1_l\"] = cat_le.transform(validation[\"cat1\"])\n",
        "validation[\"cat2_l\"] = cat_le.transform(validation[\"cat2\"])\n",
        "validation[\"cat3_l\"] = cat_le.transform(validation[\"cat3\"])\n",
        "\n",
        "validation[\"brand_name_l\"] = brand_le.transform(validation[\"brand_name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whePTuwQ7n1l"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kzLzFGi7hA0"
      },
      "source": [
        "test[\"cat1_l\"] = cat_le.transform(test[\"cat1\"])\n",
        "test[\"cat2_l\"] = cat_le.transform(test[\"cat2\"])\n",
        "test[\"cat3_l\"] = cat_le.transform(test[\"cat3\"])\n",
        "\n",
        "test[\"brand_name_l\"] = brand_le.transform(test[\"brand_name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl7czt1455gD"
      },
      "source": [
        "# Categorical only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5_3dIEXzhhF"
      },
      "source": [
        "# avoid to mess up original train and validation\n",
        "train_cat = train\n",
        "validation_cat = validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEtjrzW86PpZ"
      },
      "source": [
        "def getModelCat():\n",
        "    inputA = Input(shape=(6,))\n",
        "\n",
        "    x = Dense(32, activation='relu')(inputA)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=inputA, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oANXklrj6P2g"
      },
      "source": [
        "model_cat = getModelCat()\n",
        "model_cat.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSj_KBll9wWp"
      },
      "source": [
        "plot_model(model_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psa0HZfX9wWp"
      },
      "source": [
        "model_cat.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BgFwva-eT7"
      },
      "source": [
        "inputA_train_cat = train_cat[[\"item_condition_id\", \"cat1_l\", \"cat2_l\", \"cat3_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnpfLh6d-q8r"
      },
      "source": [
        "inputA_validation_cat = validation_cat[[\"item_condition_id\", \"cat1_l\", \"cat2_l\", \"cat3_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rwMERNQ-75F"
      },
      "source": [
        "y_train_cat = train_cat[\"price\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsJx3Uzt_Apd"
      },
      "source": [
        "y_validation_cat = validation_cat[\"price\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJual7EL9wWp"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "history_cat = model_cat.fit(x=inputA_train_cat, y=y_train_cat,\n",
        "                    epochs=10,\n",
        "                    verbose=True,\n",
        "                    validation_data=(inputA_validation_cat, y_validation_cat),\n",
        "                    callbacks=[callback],\n",
        "                    batch_size=512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1RY6BUtBxNh"
      },
      "source": [
        "10/10 2261/2261 [==============================] - 5s 2ms/step - loss: 0.6769 - mse: 1529.0349 - mae: 15.5510 - root_mean_squared_error: 39.1006 - mean_squared_logarithmic_error: 0.4589 - root_mean_squared_logarithmic_error: 0.6769 - val_loss: 0.6703 - val_mse: 1456.1018 - val_mae: 15.3929 - val_root_mean_squared_error: 38.1589 - val_mean_squared_logarithmic_error: 0.4501 - val_root_mean_squared_logarithmic_error: 0.6705\n",
        "\n",
        "CHANGED. this was the one with single category\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfUDgH0D9wWq"
      },
      "source": [
        "histDf_cat = pd.DataFrame(history_cat.history)\n",
        "histDf_cat.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al4hG-g33lq8"
      },
      "source": [
        "# Keras Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF8wNMPU1Z83"
      },
      "source": [
        "name_length_max=10\n",
        "desc_length_max=75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yehut2ut0PoR"
      },
      "source": [
        "train_keras = train\n",
        "validation_keras = validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3fBpRMpHJoh"
      },
      "source": [
        "# tokenize with keras; it also does some encoding\n",
        "def tokenizeData(df, description, name, tokenizer= None, texts= None):\n",
        "  if tokenizer == None:\n",
        "    tokenizer = Tokenizer()\n",
        "\n",
        "    print(\"fit tokenizer\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "  \n",
        "  print('tokenize description')\n",
        "  df[\"item_description_t\"]=tokenizer.texts_to_sequences(df[description])\n",
        "\n",
        "  print('tokenize name')\n",
        "  df[\"name_t\"]=tokenizer.texts_to_sequences(df[name])\n",
        "  return df, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hzwtYpKHon7"
      },
      "source": [
        "texts = np.hstack([train_keras[\"item_description_clean\"], train[\"name_clean\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Z6pk-80gJl"
      },
      "source": [
        "train_keras, tokenizer = tokenizeData(\n",
        "    train_keras ,\"item_description_clean\" ,\"name_clean\", None, texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4pyiN8I0jMX"
      },
      "source": [
        "validation_keras, _ = tokenizeData(\n",
        "    validation_keras ,\"item_description_clean\" ,\"name_clean\", tokenizer, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W0A95PQEDb_"
      },
      "source": [
        "vocab_size= len(tokenizer.word_index)+1\n",
        "print(vocab_size)\n",
        "# clean con trattamento emoji 193300\n",
        "# clean 246054\n",
        "# no clean 255431 (considerando tutto anche punteggiatura)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVKaHBR0QPgs"
      },
      "source": [
        "train_keras[[\"item_description_t\",\"name_t\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y40ov-ULHtla"
      },
      "source": [
        "inputDesc_train_keras = pad_sequences(train_keras[\"item_description_t\"],\n",
        "                                                  padding='post', maxlen=desc_length_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tky1YzlkB8U5"
      },
      "source": [
        "inputDesc_validation_keras = pad_sequences(validation_keras[\"item_description_t\"],\n",
        "                                                  padding='post', maxlen=desc_length_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBZIAP9n3yTU"
      },
      "source": [
        "inputName_train_keras = pad_sequences(train_keras[\"name_t\"], padding='post', maxlen=name_length_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uhYXRYQCCHb"
      },
      "source": [
        "inputName_validation_keras = pad_sequences(validation_keras[\"name_t\"], padding='post', maxlen=name_length_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFlm8Qqx-Qab"
      },
      "source": [
        "y_train_keras = train_keras[\"price\"]\n",
        "y_train_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkHBOiRXggJw"
      },
      "source": [
        "y_validation_keras = validation_keras[\"price\"]\n",
        "y_validation_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZh4kWw73yTY"
      },
      "source": [
        "inputA_train_keras = train_keras[[\"item_condition_id\", \"cat1_l\", \"cat2_l\", \"cat3_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oTri-SwCHqe"
      },
      "source": [
        "inputA_validation_keras = validation_keras[[\"item_condition_id\", \"cat1_l\", \"cat2_l\", \"cat3_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZmQxJaT3yTX"
      },
      "source": [
        "name_vocab_size= vocab_size\n",
        "desc_vocab_size= vocab_size\n",
        "def getModelKeras(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size):\n",
        "    inputA = Input(shape=(6,))\n",
        "    #Ad = Dense(4, activation='relu')(inputA)\n",
        "    Ad = inputA\n",
        "    \n",
        "    inputName = Input(shape=(name_length_max,))\n",
        "    Ne = Embedding(input_dim=name_vocab_size, output_dim=50, input_length=name_length_max)(inputName)\n",
        "    #Ne = Embedding(\n",
        "    #    num_tokens,\n",
        "    #    embedding_dim,\n",
        "    #    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    #    trainable=False,\n",
        "    #)(inputName)\n",
        "    Nd = Bidirectional(LSTM(12, return_sequences=True, dropout=0.2))(Ne)\n",
        "    Nd =  GlobalMaxPool1D()(Nd)\n",
        "\n",
        "    #Nd = Nf\n",
        "    \n",
        "    inputDesc = Input(shape=(desc_length_max,))\n",
        "    De = Embedding(input_dim=desc_vocab_size, output_dim=50, input_length=desc_length_max)(inputDesc)\n",
        "    #De = Embedding(\n",
        "    #    num_tokens,\n",
        "    #    embedding_dim,\n",
        "    #    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    #    trainable=False,\n",
        "    #)(inputDesc)\n",
        "    Dd = Bidirectional(LSTM(16, return_sequences=True, dropout=0.2))(De)\n",
        "    Dd = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(Dd)\n",
        "    \n",
        "    Dd =  GlobalMaxPool1D()(Dd)\n",
        "    #Dd = Df\n",
        "    \n",
        "    concat = Concatenate()([Ad, Nd, Dd])\n",
        "\n",
        "    x = Dropout(0.2)(concat)    \n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[inputA, inputName, inputDesc], outputs=x)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWfolHUC3yTX"
      },
      "source": [
        "model_keras = getModelKeras(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size)\n",
        "model_keras.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNoRvN6T3yTX"
      },
      "source": [
        "plot_model(model_keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-rcb1aLIHPX"
      },
      "source": [
        "model_keras.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQBhACJOZSf"
      },
      "source": [
        "https://machinelearningmastery.com/clean-text-machine-learning-python/ ultima sezione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2m3WcI-JWcP"
      },
      "source": [
        "callbacks = [\n",
        "             tf.keras.callbacks.EarlyStopping(\n",
        "                  monitor='val_loss',\n",
        "                  patience=3,\n",
        "                  restore_best_weights=True\n",
        "                  )\n",
        "             ]\n",
        "history_keras = model_keras.fit(x=[inputA_train_keras,inputName_train_keras, inputDesc_train_keras], y=y_train_keras,\n",
        "                    epochs=10,\n",
        "                    verbose=True,\n",
        "                    validation_data=([inputA_validation_keras ,inputName_validation_keras, inputDesc_validation_keras], y_validation_keras),\n",
        "                    callbacks=[callback],\n",
        "                    batch_size=512)\n",
        "# senza pulizia  loss: 0.4100  e val_root_mean_squared_logarithmic_error: 0.4543\n",
        "# con pulizia leggera (lower)  val_root_mean_squared_logarithmic_error: 0.455 e loss: 0.4\n",
        "# con pulizia val_root_mean_squared_logarithmic_error: 0.4643 e loss: 0.4269\n",
        "# con lunghezza embedding 50 e senza pulizia val_root_mean_squared_logarithmic_error: 0.4556 e loss: 0.3675 con 10 epoche "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ZqDQxG4de0"
      },
      "source": [
        "histDf_keras = pd.DataFrame(history_keras.history)\n",
        "histDf_keras.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8NNZQXuht7t"
      },
      "source": [
        "histDf_keras.plot(y=[\"loss\", \"val_loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdorusqa1sBT"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClNGIbYoE416"
      },
      "source": [
        "test_keras, _ = tokenizeData(test ,\"item_description\", \"name\", tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srQCadR3Jvfu"
      },
      "source": [
        "test_keras=test[[\"item_condition_id\",\"shipping\",\"category_name_l\",\"brand_name_l\", \"item_description_t\", \"name_t\"]]\n",
        "test_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgCkkae3Kdmr"
      },
      "source": [
        "inputA_test_keras = test_keras[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')\n",
        "inputName_test_keras = pad_sequences(test_keras[\"name_t\"], padding='post', maxlen=name_length_max)\n",
        "inputDesc_test_keras = pad_sequences(test_keras[\"item_description_t\"], padding='post', maxlen=desc_length_max)\n",
        "pred_keras = model_keras.predict([inputA_test_keras, inputName_test_keras, inputDesc_test_keras])\n",
        "pred_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-IkK1IkNqXD"
      },
      "source": [
        "import math\n",
        "\n",
        "def rmsle(y, y_pred):\n",
        "    assert len(y) == len(y_pred)\n",
        "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
        "    return (sum(to_sum) * (1.0/len(y))) ** 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeCyj4NcNtuU"
      },
      "source": [
        "validation_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODIe2aySMUZj"
      },
      "source": [
        "pred_keras=pred_keras.round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLp_8poJQBmn"
      },
      "source": [
        "pred_v_keras = model.predict([inputA_validation_keras, inputName_validation_keras, inputDesc_validation_keras])\n",
        "pred_v_keras = pred_v_keras.round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NmUm_gaRWHL"
      },
      "source": [
        "len(pred_v_keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFrRfXG-RhVr"
      },
      "source": [
        "len(y_validation_keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJPxtmcQRJlp"
      },
      "source": [
        "root_mean_squared_logarithmic_error(y_validation_keras, pred_v_keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G91ZG_SXUU50"
      },
      "source": [
        "# Glove pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiRTLZIl2DGJ"
      },
      "source": [
        "train_glove = train\n",
        "validation_glove = validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNBwsIkTJtuu"
      },
      "source": [
        "https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaYOtsY36rOz"
      },
      "source": [
        "#archive_url = 'http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip'\n",
        "#archive_url = \"https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\"\n",
        "#archive_url = \"https://www.cs.uic.edu/~hxu/ele_review_qa_300d.tar.gz\"\n",
        "archive_url = 'http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.840B.300d.zip'\n",
        "archive_name = 'glove.840B.300d.zip'\n",
        "remove_archive = False\n",
        "embedding_fname = 'glove.840B.300d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUeT17cO-xhP"
      },
      "source": [
        "if not os.path.isfile(archive_name) and not os.path.isfile(embedding_fname):\n",
        "  ! wget {archive_url} -O {archive_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6X2PUTqGNGI"
      },
      "source": [
        "if not os.path.isfile(embedding_fname):\n",
        "  ! unzip {archive_name} {embedding_fname}\n",
        "\n",
        "# remove archive if already extracted\n",
        "if remove_archive and os.path.isfile(embedding_fname):\n",
        "  os.remove(archive_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTMOU0Pk8A6_"
      },
      "source": [
        "Needs a word encoding index (e.g. keras one)\n",
        "\n",
        "TODO: creare un word index da qui se non c'è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPFLTtl_C6A4"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVnTcD3nCet7"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    base_dir, embedding_fname\n",
        ")\n",
        "\n",
        "'''embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg-i_nmregk7"
      },
      "source": [
        "path_to_glove_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYVbr8mGYHqd"
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index (padding)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "              i = word_index[word]\n",
        "              try: \n",
        "                embedding_matrix[i] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "              except:\n",
        "                print(word)\n",
        "                print(vector)\n",
        "                print(\"_______________________\")\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HqTmXgE_P8t"
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index (padding)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "              i = word_index[word]\n",
        "              try: \n",
        "                embedding_matrix[i] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "                \n",
        "  \n",
        "              except:\n",
        "\n",
        "                vector=vector[1:embedding_dim+1]\n",
        "                embedding_matrix[i] = np.array(vector, dtype=np.float32)\n",
        "\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16SQunrgYXv2"
      },
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(path_to_glove_file, tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp2dcxR2fi_n"
      },
      "source": [
        "embedding_matrix[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZBiI8g4eiz8"
      },
      "source": [
        "embedding_matrix[107098]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqusQItvZDmS"
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size\n",
        "# senza pulizia viene coperto il 28% del vocabolario\n",
        "# con pulizia il 26% senza emoji\n",
        "# con pulizia con emoji 34%\n",
        "# 0.3779 senza pulizia con glove più grande (glove.840B.300d)\n",
        "# 0.467 con pulizia con glove più grande (glove.840B.300d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vffbp5FHB-ZN"
      },
      "source": [
        "'''#glove pretrained embedding\n",
        "num_tokens = len(word_index) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4CwmHQl-zF"
      },
      "source": [
        "**IMPORTANTISSIMO: SCEGLIERE CHE LUNGHEZZA USARE PER L'ECONDING DELLE PAROLE DELLE DESCRIZIONI E DEI NOMI, CIOE' SE 8 VALORI ENTRAMBI O MANTENERE 8 E 16 VALORI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy937m-Yh-3T"
      },
      "source": [
        "name_vocab_size= vocab_size\n",
        "desc_vocab_size= vocab_size\n",
        "def getModelGlove(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size, embedding_matrix=None):\n",
        "    inputA = Input(shape=(4,))\n",
        "    Ad = inputA\n",
        "    \n",
        "    inputName = Input(shape=(name_length_max,))\n",
        "    if embedding_matrix.size == 0:\n",
        "      Ne = Embedding(input_dim=name_vocab_size, output_dim=50, weights=[embedding_matrix], input_length=name_length_max, trainable=False)(inputName)\n",
        "    else: \n",
        "      Ne = Embedding(input_dim=name_vocab_size, output_dim=50, input_length=name_length_max)(inputName)\n",
        "\n",
        "    Nd = Bidirectional(LSTM(12, return_sequences=True, dropout=0.2))(Ne)\n",
        "    Nd =  GlobalMaxPool1D()(Nd)\n",
        "    \n",
        "    inputDesc = Input(shape=(desc_length_max,))\n",
        "    if embedding_matrix.size == 0:\n",
        "      De = Embedding(input_dim=desc_vocab_size, output_dim=50, weights=[embedding_matrix], input_length=desc_length_max, trainable=False)(inputDesc)\n",
        "    else:\n",
        "      De = Embedding(input_dim=desc_vocab_size, output_dim=50, input_length=desc_length_max)(inputDesc)\n",
        "\n",
        "\n",
        "    Dd = Bidirectional(LSTM(16, return_sequences=True, dropout=0.2))(De)\n",
        "    Dd = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(Dd)\n",
        "    \n",
        "    Dd =  GlobalMaxPool1D()(Dd)\n",
        "    \n",
        "    concat = Concatenate()([Ad, Nd, Dd])\n",
        "\n",
        "    x = Dropout(0.2)(concat)    \n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[inputA, inputName, inputDesc], outputs=x)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh6pXWdkhbzZ"
      },
      "source": [
        "model_glove= getModelGlove(name_length_max, desc_length_max, name_vocab_size, desc_vocab_size, embedding_matrix)\n",
        "model_glove.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVVDyURj6ey"
      },
      "source": [
        "plot_model(model_glove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeBKCNokAoL"
      },
      "source": [
        "model_glove.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnI4K0UG2b31"
      },
      "source": [
        "TODO input glove and not keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9QznRskJP-"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "history_glove = model_glove.fit(x=[inputA_train_keras,inputName_train_keras, inputDesc_train_keras], y=y_train_keras,\n",
        "                    epochs=10,\n",
        "                    verbose=True,\n",
        "                    validation_data=([inputA_validation_keras ,inputName_validation_keras, inputDesc_validation_keras], y_validation_keras),\n",
        "                    callbacks=[callback],\n",
        "                    batch_size=512)\n",
        "# glove 6M \n",
        "# con 10 epoche e senza pulizia: loss: 0.5564 e val_root_mean_squared_logarithmic_error: 0.5392\n",
        "# con 10 epoche e pulizia emoji etc: loss: 0.5553 e val_root_mean_squared_logarithmic_error: 0.5312\n",
        "\n",
        "# glove.840B.300d\n",
        "# con 10 epoche senza pulizia: loss: 0.3678  e val_root_mean_squared_logarithmic_error: 0.4537\n",
        "# con 10 epoce con pulizia: loss: 0.3817 e val_root_mean_squared_logarithmic_error 0.4604"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am9eiXhqax3l"
      },
      "source": [
        "# BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMvcp3DzpFlE"
      },
      "source": [
        "train_cvec = train\n",
        "validation_cvec = validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc9UaX5XKaRg"
      },
      "source": [
        "categorical_train_cvec= train_cvec[[\"item_condition_id\",\"shipping\", \"cat1_l\", \"cat2_l\", \"cat3_l\", \"brand_name_l\"]]\n",
        "print(\"Train:\")\n",
        "print(categorical_train_cvec.head())\n",
        "print(\"\\nValidation:\")\n",
        "categorical_validation_cvec= validation_cvec[[\"item_condition_id\",\"shipping\",\"cat1_l\", \"cat2_l\", \"cat3_l\",\"brand_name_l\"]]\n",
        "print(categorical_validation_cvec.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I1r2JgCxP1g"
      },
      "source": [
        "y_train_cvec=train_cvec[\"price\"].values\n",
        "y_validation_cvec=validation_cvec[\"price\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMtKfPMrZBk"
      },
      "source": [
        "## Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnLBxl6b0Ruj"
      },
      "source": [
        "vectorizer_desc = CountVectorizer()\n",
        "vectorizer_desc.fit(train_cvec[\"item_description\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hGHK3_f3du3"
      },
      "source": [
        "len(vectorizer_desc.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvNUFh5zFz09"
      },
      "source": [
        "description_train_cvec = vectorizer_desc.transform(train_cvec[\"item_description\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etpa0ospxMm"
      },
      "source": [
        "description_validation_cvec = vectorizer_desc.transform(validation_cvec[\"item_description\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myswCjJVeB6"
      },
      "source": [
        "type(description_validation_cvec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRsUcwbSpTWu"
      },
      "source": [
        "vectorizer_name = CountVectorizer()\n",
        "vectorizer_name.fit(train_cvec[\"name\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx3VVZ15qI0h"
      },
      "source": [
        "len(vectorizer_name.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh1PddSiqSS_"
      },
      "source": [
        "name_train_cvec = vectorizer_name.transform(train_cvec[\"name\"].values)\n",
        "name_validation_cvec = vectorizer_name.transform(validation_cvec[\"name\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK01Kzkj6244"
      },
      "source": [
        "type(name_train_cvec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzdkPXSu4VMb"
      },
      "source": [
        "def getModel_bow(categorical_train, name_train, description_train):\n",
        "    inputA = Input(categorical_train.shape[1])\n",
        "    inputName = Input(name_train.shape[1])\n",
        "    inputDesc = Input(description_train.shape[1])\n",
        "    concat = Concatenate()([inputName, inputDesc, inputA])\n",
        "\n",
        "    x = Dropout(0.1)(concat)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[inputName ,inputDesc, inputA], outputs=x)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd_anq_Cxg4h"
      },
      "source": [
        "model_cvec = getModel_bow(categorical_train_cvec, name_train_cvec, description_train_cvec)\n",
        "model_cvec.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIsxSCOxyzLb"
      },
      "source": [
        "plot_model(model_cvec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93QpCY4F3hXa"
      },
      "source": [
        "model_cvec.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B70Cb3AW7sEw"
      },
      "source": [
        "history_cvec = model_cvec.fit(x=[name_train_cvec, description_train_cvec, categorical_train_cvec.values],\n",
        "                    y=y_train_cvec,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=(\n",
        "                        [name_validation_cvec, description_validation_cvec, categorical_validation_cvec.values]\n",
        "                        , y_validation_cvec),\n",
        "                    batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uHhyAV_VXCw"
      },
      "source": [
        "BOW con pulizia:\n",
        "loss: 0.4549 - mse: 877.1694 - mae: 10.7754 - root_mean_squared_error: 29.6144 - mean_squared_logarithmic_error: 0.2073 - root_mean_squared_logarithmic_error: 0.4549 - val_loss: 0.4572 - val_mse: 823.4496 - val_mae: 10.7306 - val_root_mean_squared_error: 28.6958 - val_mean_squared_logarithmic_error: 0.2095 - val_root_mean_squared_logarithmic_error: 0.4573"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbJJkuirlZz1"
      },
      "source": [
        "BOW senza pulizia words:\n",
        "loss: 0.4537 - mse: 892.7444 - mae: 10.7434 - root_mean_squared_error: 29.8743 - mean_squared_logarithmic_error: 0.2063 - root_mean_squared_logarithmic_error: 0.4537 - val_loss: 0.4554 - val_mse: 848.4371 - val_mae: 10.7117 - val_root_mean_squared_error: 29.1279 - val_mean_squared_logarithmic_error: 0.2079 - val_root_mean_squared_logarithmic_error: 0.4555"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf88pKGe_6FI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    rmsle = history.history['root_mean_squared_logarithmic_error']\n",
        "    val_rmsle = history.history['val_root_mean_squared_logarithmic_error']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(rmsle) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, rmsle, 'b', label='Training rmsle')\n",
        "    plt.plot(x, val_rmsle, 'r', label='Validation rmsle')\n",
        "    plt.title('Training and validation root_mean_squared_logarithmic_error')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzFRzm-__Se"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uusyBK_m75L"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0qBBoukwa_K"
      },
      "source": [
        "data[\"item_description\"] = data[\"item_description\"]\n",
        "data[\"name\"] = data[\"name\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHvnuhG_nF-o"
      },
      "source": [
        "tfidf_vectorizer_desc = TfidfVectorizer()\n",
        "tfidf_vectorizer_desc.fit(train_cvec[\"item_description\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNsnLmuEnOCA"
      },
      "source": [
        "len(tfidf_vectorizer_desc.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FEV_JeYnOq7"
      },
      "source": [
        "tfidf_description_train = tfidf_vectorizer_desc.transform(train_cvec[\"item_description\"].values)\n",
        "tfidf_description_validation = tfidf_vectorizer_desc.transform(validation_cvec[\"item_description\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-3628USnioQ"
      },
      "source": [
        "tfidf_vectorizer_name = TfidfVectorizer()\n",
        "tfidf_vectorizer_name.fit(train_cvec[\"name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-qOh3FXnzSb"
      },
      "source": [
        "len(tfidf_vectorizer_name.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOnnouCxn_he"
      },
      "source": [
        "tfidf_name_train = tfidf_vectorizer_name.transform(train_cvec[\"name\"])\n",
        "tfidf_name_validation = tfidf_vectorizer_name.transform(validation_cvec[\"name\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCSrpsML5e47"
      },
      "source": [
        "def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    coo = X.tocoo()\n",
        "    indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    return tf.SparseTensor(indices, coo.data, coo.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO23VoG_Umkx"
      },
      "source": [
        "type(tfidf_name_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyGnExtwqcgx"
      },
      "source": [
        "#https://stackoverflow.com/questions/61961042/indices201-0-8-is-out-of-order-many-sparse-ops-require-sorted-indices-use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHw1A89wXWw0"
      },
      "source": [
        "import scipy\n",
        "new_categorical_train_cvec=scipy.sparse.csr_matrix(categorical_train_cvec.values)\n",
        "new_categorical_validation_cvec=scipy.sparse.csr_matrix(categorical_validation_cvec.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeLdc-Esipcg"
      },
      "source": [
        "new_tfidf_categorical_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_categorical_validation_cvec))\n",
        "new_tfidf_categorical_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_categorical_train_cvec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsRkzDbNhVPN"
      },
      "source": [
        "new_tfidf_name_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_name_validation))\n",
        "new_tfidf_name_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_name_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tXBiPsZiE19"
      },
      "source": [
        "type(new_tfidf_name_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4XHrIyGiHS_"
      },
      "source": [
        "type(tfidf_name_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqX5vyv5jMjJ"
      },
      "source": [
        "new_tfidf_description_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_description_validation))\n",
        "new_tfidf_description_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(tfidf_description_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rBPl6RDX7qM"
      },
      "source": [
        "'''new_y_train_cvec=scipy.sparse.csr_matrix(y_train_cvec)\n",
        "new_y_validation_cvec=scipy.sparse.csr_matrix(y_validation_cvec)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOJzQCvZi1MH"
      },
      "source": [
        "'''new_tfidf_y_validation=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_validation_cvec))\n",
        "new_tfidf_y_train=tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_train_cvec))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBQK0C3bWkQs"
      },
      "source": [
        "'''type(tf.sparse.reorder(convert_sparse_matrix_to_sparse_tensor(new_y_train_cvec)))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKQCzVxn9GT1"
      },
      "source": [
        "def getModel_bow(name_train, description_train, categorical_train):\n",
        "    inputA = Input(categorical_train.shape[1])\n",
        "    inputName = Input(name_train.shape[1])\n",
        "    inputDesc = Input(description_train.shape[1])\n",
        "    concat = Concatenate()([inputName, inputDesc, inputA])\n",
        "\n",
        "    #x = Dropout(0.1)(concat)\n",
        "    x = Dense(32, activation='relu')(concat)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[inputName ,inputDesc, inputA], outputs=x)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XK-E2e0oKeC"
      },
      "source": [
        "model = getModel_bow(new_tfidf_name_train, new_tfidf_description_train, categorical_validation_cvec)\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqJ7pgfosi4q"
      },
      "source": [
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Fv4q35ocWN"
      },
      "source": [
        "model.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a80wIgkOoelZ"
      },
      "source": [
        "history = model.fit(x=[new_tfidf_name_train, new_tfidf_description_train, new_tfidf_categorical_train], y=y_train_cvec,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=([new_tfidf_name_validation, new_tfidf_description_validation, new_tfidf_categorical_validation], y_validation_cvec),\n",
        "                    batch_size=512)\n",
        "# 0.4771"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RBlRb_gujSz"
      },
      "source": [
        "history2 = model.fit(x=[new_tfidf_name_train, new_tfidf_description_train], y=y_train_cvec,\n",
        "                    epochs=5,\n",
        "                    verbose=True,\n",
        "                    validation_data=([new_tfidf_name_validation, new_tfidf_description_validation], y_validation_cvec),\n",
        "                    batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3QDoGQvxpCl"
      },
      "source": [
        "histdf = pd.DataFrame(history2.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH8wryVvxtaA"
      },
      "source": [
        "histdf.plot(y=[\"loss\", \"val_loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIrZwW2wq7r7"
      },
      "source": [
        "tf-idf gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPe1LMPVq9ln"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-0p3DMritP"
      },
      "source": [
        "data[\"item_description\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CO2a-oguycH"
      },
      "source": [
        "doc_tokenized = [simple_preprocess(doc) for doc in data[\"item_description\"].values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDjGjel3vHhk"
      },
      "source": [
        "doc_tokenized[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8L2nbDrvQly"
      },
      "source": [
        "dictionary = corpora.Dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gBzeihTvVom"
      },
      "source": [
        "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LafgQkjqv4dV"
      },
      "source": [
        "# stampa le parole che compaiono in ogni descrizione con la loro frequenza di apparizione\n",
        "for doc in BoW_corpus[:10]:\n",
        "   print([[dictionary[id], freq] for id, freq in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYA8xxQOyabr"
      },
      "source": [
        "tfidf = TfidfModel(BoW_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VjEh4WP0loV"
      },
      "source": [
        "for doc in tfidf[BoW_corpus]:\n",
        "   print([[dictionary[id], np.around(freq)] for id, freq in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tFxY7se1Uyo"
      },
      "source": [
        "data[\"tfidf\"]=tfidf[BoW_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCbOqhV03DIX"
      },
      "source": [
        "len(data.iloc[10][\"tfidf\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0CKj8s30wY"
      },
      "source": [
        "train_tfidf, validation_tfidf = train_test_split(data[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\", \"tfidf\", \"price\"]], test_size=0.2, random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aG4lyxj50R5"
      },
      "source": [
        "train_tfidf_categorical=train_tfidf[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\"]]\n",
        "validation_tfidf_categorical=validation_tfidf[[\"item_condition_id\", \"shipping\", \"category_name_l\",\t\"brand_name_l\"]]\n",
        "\n",
        "\n",
        "train_tfidf_description=train_tfidf[\"tfidf\"]\n",
        "validation_tfidf_description=validation_tfidf[\"tfidf\"]\n",
        "\n",
        "y_train=train_tfidf[\"price\"]\n",
        "y_validation=validation_tfidf[\"price\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_507TTwr6rmz"
      },
      "source": [
        "(validation_tfidf_description[0:10].todense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCIuh8-24ied"
      },
      "source": [
        "def getModel_bow(categorical_train, description_train):\n",
        "    inputA = Input(categorical_train.shape[1])\n",
        "    inputDesc = Input(description_train.shape[1])\n",
        "    concat = Concatenate()([inputDesc, inputA])\n",
        "\n",
        "    x = Dropout(0.1)(concat)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[inputDesc, inputA], outputs=x)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUUxfAFd4xGk"
      },
      "source": [
        "model = getModel_bow(train_tfidf_categorical ,train_tfidf_description)\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3YG-zBS7-mp"
      },
      "source": [
        "train_tfidf_description.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE4hyzsKBQRu"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDO1it7hDc8P"
      },
      "source": [
        "train_trans_all = train\n",
        "validation_trans = validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLeogNaNBe6J"
      },
      "source": [
        "pret_model_trans = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "pret_model_trans.trainable = False\n",
        "\n",
        "tokenizer_trans = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', use_fast=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX_KoNI_1k4G"
      },
      "source": [
        "instances_per_batch = 200000\n",
        "\n",
        "num_batches = int(train_trans_all.shape[0] / instances_per_batch)\n",
        "\n",
        "trans_batch_starti = list(range(0, train_trans_all.shape[0], int(train_trans_all.shape[0] / num_batches)))[0:num_batches]\n",
        "trans_batch_starti.append(train_trans_all.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-9cTcBV19U0"
      },
      "source": [
        "trans_batches = [(trans_batch_starti[i], trans_batch_starti[i+1]) for i in range(0,len(trans_batch_starti)-1)]\n",
        "[(i,b) for i,b in enumerate(trans_batches)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmjfqcS310JG"
      },
      "source": [
        "# divide in batch as we don't have enough memory to handle all at once\n",
        "# 0 to 9\n",
        "trans_batch_number = 0\n",
        "\n",
        "trans_batch = trans_batches[trans_batch_number]\n",
        "trans_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgUJwyC2pigv"
      },
      "source": [
        "print(\"Total train shape\", train_trans_all.shape)\n",
        "train_trans = train_trans_all[trans_batch[0]:trans_batch[1]]\n",
        "print(\"Train batch shape\", train_trans.shape)\n",
        "print(\"Validation shape\", validation_trans.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQCPY-sSjDy"
      },
      "source": [
        "y_train_trans = train_trans[\"price\"]\n",
        "y_train_trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmlNBK-aSn0L"
      },
      "source": [
        "y_validation_trans = validation_trans[\"price\"]\n",
        "y_validation_trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpeCVE5xBJnx"
      },
      "source": [
        "todo: use cleaned? or let bert handle everything?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZnPt2pwPhda"
      },
      "source": [
        "inputA_train_trans = train_trans[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1vQL1C1Pchc"
      },
      "source": [
        "inputA_validation_trans = validation_trans[[\"item_condition_id\", \"category_name_l\", \"brand_name_l\", \"shipping\"]].values.astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEQjK8h4l0mg"
      },
      "source": [
        "trans_name_tokenizer_maxlength = 50\n",
        "trans_desc_tokenizer_maxlength = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEeOZukjBF3i"
      },
      "source": [
        "inputName_train_trans = tokenizer_trans(train_trans[\"name\"].to_list(),\n",
        "                                  return_tensors=\"tf\",\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  max_length = trans_name_tokenizer_maxlength)\n",
        "inputName_train_trans['input_ids'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ8sh5pUPrIC"
      },
      "source": [
        "inputName_validation_trans = tokenizer_trans(validation_trans[\"name\"].to_list(),\n",
        "                                  return_tensors=\"tf\",\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  max_length = trans_name_tokenizer_maxlength)\n",
        "inputName_validation_trans['input_ids'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bQnsc9-JcIq"
      },
      "source": [
        "inputName_shape_trans = (inputName_train_trans['input_ids'].shape[1],\n",
        "                         inputName_train_trans['attention_mask'].shape[1])\n",
        "\n",
        "inputName_shape_trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMjIqkOs2jhU"
      },
      "source": [
        "inputDesc_train_trans = tokenizer_trans(train_trans[\"item_description_clean\"].to_list(),\n",
        "                                  return_tensors=\"tf\",\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  max_length=trans_desc_tokenizer_maxlength)\n",
        "inputDesc_train_trans['input_ids'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZccJ_0HUP5EX"
      },
      "source": [
        "inputDesc_validation_trans = tokenizer_trans(validation_trans[\"item_description_clean\"].to_list(),\n",
        "                                  return_tensors=\"tf\",\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  max_length=trans_desc_tokenizer_maxlength)\n",
        "inputDesc_validation_trans['input_ids'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCTEIDnSJsMP"
      },
      "source": [
        "inputDesc_shape_trans = (inputDesc_train_trans['input_ids'].shape[1],\n",
        "                         inputDesc_train_trans['attention_mask'].shape[1])\n",
        "inputDesc_shape_trans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_odyHkIAJWy"
      },
      "source": [
        "def getModel_trans():\n",
        "    inputA = Input(shape=(4,))\n",
        "    Ad = inputA    \n",
        "    \n",
        "    inputName_ids = Input(shape=(inputName_shape_trans[0],), dtype='int32')\n",
        "    inputName_mask = Input(shape=(inputName_shape_trans[1],), dtype='int32')\n",
        "\n",
        "    Np = pret_model_trans(inputName_ids, attention_mask=inputName_mask)[0]\n",
        "\n",
        "    Nd = Bidirectional(LSTM(12, return_sequences=True, dropout=0.2))(Np)\n",
        "    Nd =  GlobalMaxPool1D()(Nd)\n",
        "\n",
        "    inputDesc_ids = Input(shape=(inputDesc_shape_trans[0],), dtype='int32')\n",
        "    inputDesc_mask = Input(shape=(inputDesc_shape_trans[1],), dtype='int32')\n",
        "\n",
        "    Dp = pret_model_trans(inputDesc_ids, attention_mask=inputDesc_mask)[0]\n",
        "\n",
        "    Dd = Bidirectional(LSTM(16, return_sequences=True, dropout=0.2))(Dp)\n",
        "    Dd = Bidirectional(LSTM(8, return_sequences=True, dropout=0.2))(Dd)\n",
        "   \n",
        "    Dd = GlobalMaxPool1D()(Dd)\n",
        "\n",
        "    concat = Concatenate()([Ad, Nd, Dd])\n",
        "\n",
        "    x = Dropout(0.2)(concat)    \n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    \n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    \n",
        "    model = Model(inputs=[\n",
        "                          inputA,\n",
        "                          inputName_ids,\n",
        "                          inputName_mask,\n",
        "                          inputDesc_ids,\n",
        "                          inputDesc_mask\n",
        "                          ], outputs=x)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN9opFTBHLXu"
      },
      "source": [
        "model_trans = getModel_trans()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdOujyB1HcZ3"
      },
      "source": [
        "model_trans.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e00K_QQ8O6Nq"
      },
      "source": [
        "plot_model(model_trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyqut5HjPBR4"
      },
      "source": [
        "model_trans.compile(loss = root_mean_squared_logarithmic_error, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError(), 'mean_squared_logarithmic_error', root_mean_squared_logarithmic_error])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV6080wZPLlo"
      },
      "source": [
        "history_trans = model_trans.fit(x=[inputA_train_trans,\n",
        "                   inputName_train_trans['input_ids'],\n",
        "                   inputName_train_trans['attention_mask'],\n",
        "                   inputDesc_train_trans['input_ids'],\n",
        "                   inputDesc_train_trans['attention_mask']\n",
        "                  ],\n",
        "                   y=y_train_trans,\n",
        "                    epochs=10,\n",
        "                    verbose=True,\n",
        "                    validation_data=([\n",
        "                                      inputA_validation_trans,\n",
        "                                      inputName_validation_trans['input_ids'],\n",
        "                                      inputName_validation_trans['attention_mask'],\n",
        "                                      inputDesc_validation_trans['input_ids'],\n",
        "                                      inputDesc_validation_trans['attention_mask'],\n",
        "                                      ], \n",
        "                                     y_validation_trans),\n",
        "                    batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6VVZIhQEzZ"
      },
      "source": [
        "453/453 [==============================] - 1733s 4s/step - loss: 0.5675 - mse: 1256.7464 - mae: 13.4240 - root_mean_squared_error: 35.4326 - mean_squared_logarithmic_error: 0.3226 - root_mean_squared_logarithmic_error: 0.5675 - val_loss: 0.5444 - val_mse: 1157.3416 - val_mae: 12.7427 - val_root_mean_squared_error: 34.0197 - val_mean_squared_logarithmic_error: 0.2970 - val_root_mean_squared_logarithmic_error: 0.5445"
      ]
    }
  ]
}